[
{
	"uri": "https://blog.codist.me/stl/set/",
	"title": "SET容器 - 自定义排序和去重",
	"tags": [],
	"description": "基于红黑树的平衡二叉树的数据结构实现的一种容器",
	"content": " std::set，是基于红黑树的平衡二叉树的数据结构实现的一种容器，因为其中所包含的元素的值是唯一的，因此主要用于排序和去重。\n1.使用内置的比较函数less 定义内置类型的set对象，限制：\n 用于比较内置类型，如int，char 只能对一个内置类型进行排序或去重  示例排序(c++11)：\n#include \u0026lt;iostream\u0026gt; #include \u0026lt;set\u0026gt; int main() { std::set\u0026lt;int\u0026gt; testSet; testSet.insert(20); testSet.insert(10); for (auto i : testSet) { std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026quot; \u0026quot;; } std::cout \u0026lt;\u0026lt; std::endl; return 0; }  Microsoft Visual C++ 6.0(c++98)：\n#include \u0026lt;iostream\u0026gt; #include \u0026lt;set\u0026gt; using namespace std; int main() { std::set\u0026lt;int\u0026gt; testSet; testSet.insert(20); testSet.insert(10); //iteratorz，迭代器，用于遍历容器内元素和元素数据类型 for (set\u0026lt;int\u0026gt;::iterator i = testSet.begin(); i != testSet.end(); i++) { cout \u0026lt;\u0026lt; *i \u0026lt;\u0026lt; \u0026quot; \u0026quot;; } cout \u0026lt;\u0026lt; endl; return 0; }  输出：\n10 20  2.定义比较函数对自定义结构体(类)排序 2.1重载操作符 \u0026lt; 不能重载\u0026lt;=或\u0026gt;=，几乎所有的方法或容器都需要排序来满足数学意义上的标准严格弱序化，否则这些方法或容器的行为将不可预知。\n严格弱序化需要满足：\n   关系 说明     f(x,x) = false 相同为假   if f(x,y) then !f(y,x) 如果 x \u0026lt; y(x \u0026gt; y) 为真，则 x \u0026gt; y (x \u0026lt; y) 为假   if f(x,y) and f(y,z) then f(x,z) 如果 x \u0026gt; y (x \u0026lt; y) 且 y \u0026gt; z (y \u0026lt; z)，则x \u0026gt; z(x \u0026lt; z)   if !f(x,y)\u0026amp;\u0026amp;!f(y,x) then x==y; if x==y and y==z then x==z 如果 x \u0026lt; y 为假且 x \u0026gt; y 为假，则 x = y    set容器在判定已有元素a和新插入元素b是否相等时：\n 1.将a作为左操作数，b为右操作数，调用比较函数，得到返回值 2.将b作为左操作数，a为右操作数，再调用一次比较函数，得到返回值 3.如果前两步返回值都为false，则a=b，b不会被插入set容器；如果前两步返回值都为true，则可能得到不可预知的结果。  由上述，必须满足：比较函数对相同元素返回false\n示例，对结构体Stu按照id排序，如果id相同，对name按照字典序排序，同时去重(c++11)：\n#include \u0026lt;cstring\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;set\u0026gt; struct Stu { int id; const char *name; Stu(int d, const char *n) : id(d), name(n) {} Stu() {} //重载运算符'\u0026lt;' bool operator\u0026lt;(const Stu \u0026amp;right) const { if (id != right.id) return id \u0026lt; right.id; else { //id相同，根据name按照字典序排序，同时去重 if (strcmp(this-\u0026gt;name, right.name) == 0) return false; else return strcmp(this-\u0026gt;name, right.name) \u0026lt; 0; } } }; int main() { std::set\u0026lt;Stu\u0026gt; testSet; testSet.insert(Stu(2, \u0026quot;zhang\u0026quot;)); testSet.insert(Stu(2, \u0026quot;li\u0026quot;)); testSet.insert(Stu(2, \u0026quot;li\u0026quot;)); //重复数据 //对应insert，先调用存储对象构造函数，在内存中生成对象，然后拷贝至容器中，c++98不支持 testSet.emplace(1, \u0026quot;wang\u0026quot;); for (auto i : testSet) { std::cout \u0026lt;\u0026lt; \u0026quot;id: \u0026quot; \u0026lt;\u0026lt; i.id \u0026lt;\u0026lt; \u0026quot; \u0026quot; \u0026lt;\u0026lt; \u0026quot;name: \u0026quot; \u0026lt;\u0026lt; i.name \u0026lt;\u0026lt; std::endl; } return 0; }  Microsoft Visual C++ 6.0(c++98)：\n#include \u0026lt;string.h\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;set\u0026gt; using namespace std; struct Stu { int id; const char *name; Stu(int d, const char *n) : id(d), name(n) {} Stu() {} //重载运算符'\u0026lt;' bool operator\u0026lt;(const Stu \u0026amp;right) const { if (id != right.id) return id \u0026lt; right.id; else { //id相同，根据name按照字典序排序，同时去重 if (strcmp(this-\u0026gt;name, right.name) == 0) return false; else return strcmp(this-\u0026gt;name, right.name) \u0026lt; 0; } } }; int main() { set\u0026lt;Stu\u0026gt; testSet; testSet.insert(Stu(2, \u0026quot;zhang\u0026quot;)); testSet.insert(Stu(2, \u0026quot;li\u0026quot;)); testSet.insert(Stu(2, \u0026quot;li\u0026quot;)); testSet.insert(Stu(1, \u0026quot;wang\u0026quot;)); for (set\u0026lt;Stu\u0026gt;::iterator i = testSet.begin(); i != testSet.end(); i++) { cout \u0026lt;\u0026lt; \u0026quot;id: \u0026quot; \u0026lt;\u0026lt; i-\u0026gt;id \u0026lt;\u0026lt; \u0026quot; \u0026quot; \u0026lt;\u0026lt; \u0026quot;name: \u0026quot; \u0026lt;\u0026lt; i-\u0026gt;name \u0026lt;\u0026lt; endl; } return 0; }  输出：\nid: 1 name: wang id: 2 name: li id: 2 name: zhang  2.2重载操作符 () "
},
{
	"uri": "https://blog.codist.me/git/gpg/",
	"title": "Git 用GPG签名",
	"tags": [],
	"description": "生成gpg key和配置用gpg签名git commit",
	"content": " 1. 生成GPG key gpg --full-generate-key   加密方式选择RSA and RSA 过期时间输入：4096 valid选择默认  填写信息后生成成功。\n2.导出公钥 gpg --list-secret-keys --keyid-format LONG  rsa4096/后面的就是 GPG key ID\ngpg --armor --export \u0026lt;要导出的GPG Key ID\u0026gt;  将得到的一大段公钥粘贴到git平台。\n3.配置git 1.设置签名用的GPG key ID\ngit config --global user.signingkey \u0026lt;GPG Key ID\u0026gt;  2.设置gpg签名用的程序\ngit config --global gpg.program gpg  3.启用gpg签名\ngit config --global commit.gpgsign true  测试签名\necho \u0026quot;test\u0026quot; | gpg --clearsign  "
},
{
	"uri": "https://blog.codist.me/asr/",
	"title": "基于kaldi的中文语音识别",
	"tags": [],
	"description": "",
	"content": " 基于kaldi的中文语音识别 用开源工具集Kaldi ASR，中文语音识别公共数据集thchs30和aishell进行训练和语音识别\n 安装kaldi  在Linux平台安装kaldi，包括cuda的配置\n   使用thchs30数据集  训练thchs30数据集的一些细节，用训练好的模型在线解码\n   使用aishell数据集  手动获取和处理aishell数据集，训练以及在线识别\n   使用cvte预训练模型  获取cvte的预训练模型并进行解码测试\n   "
},
{
	"uri": "https://blog.codist.me/asr/kaldi/",
	"title": "安装kaldi",
	"tags": ["kaldi", "ASR"],
	"description": "在Linux平台安装kaldi，包括cuda的配置",
	"content": " 环境 操作系统建议使用Ubuntu，因为官方源里已经有编译好的一些依赖库，如ATLAS，安装不会遇到太多问题。\n需要安装有git，subversion，make以及gcc等编译工具链\n1.下载源码 git clone https://github.com/kaldi-asr/kaldi.git  2.编译安装kaldi/tools 安装文档在kaldi/tools/INSTALL，很短，建议阅读一下\n开始编译：\ncd tools ./extras/check_dependencies.sh #检查依赖，如有问题参照仔细修改 #自动下载安装依赖 make -j 8  这会安装ATLAS headers, OpenFst, SCTK 和 sph2pipe\n注意：如果要使用特定版本的编译器，必须和后面编译安装kaldi/src的编译器一致，如之后我们编译安装cuda要用g++-7，需要指定要用的编译器：\n CXX=g++-7 extras/check_dependencies.sh make CXX=g++-7 -j 8  3.编译安装kaldi/src 3.1使用OpenBLAS 如果使用Ubuntu系统，源里有编译好的ATLAS库，可以直接安装，其他Linux发行版由于ATLAS安装复杂，需要调节cpu工作模式，这里用OpenBLAS替代：\ncd tools ./extras/install_openblas.sh  3.2安装cuda thchs30和aishell的训练都需要用到cuda，建议在编译安装的时候就把相关依赖一并装好。\n可以手动在nvidia官网下载cuda工具集，根据对应的显卡和平台：https://developer.nvidia.com/cuda-downloads ，再手动安装。\nArchlinux可以直接从源里安装，Ubuntu等发行版类似：\nsudo pacman -S cuda  注意：对于Archlinux，由于源里的cuda更新比较频繁，编译安装的kaldi会依赖指定版本的cuda，如果cuda大版本更新，如10.0=\u0026gt;10.1，会使kaldi找不到旧版的cuda库而出错，建议添加cuda到忽略升级里，或每次更新cuda后重新编译安装kaldi/src\n3.3编译 根据安装文档下载编译依赖(这里使用OpenBLAS)，需要硬盘上有20G的空闲空间：\n./configure --openblas-root=../tools/OpenBLAS/install  如果没有找到cuda安装路径，提示：\nCUDA will not be used! If you have already installed cuda drivers and cuda toolkit, try using --cudatk-dir=... option. Note: this is only relevant for neural net experiments  按照提示指定cuda安装路径，Archlinux安装路径在/opt/cuda/：\n./configure --openblas-root=../tools/OpenBLAS/install --cudatk-dir=/opt/cuda/  如果提示g++版本问题：\nConfiguring dynamically loaded OpenBlas since --static-math=no (the default) Successfully configured for Linux with OpenBLAS from /home/countstarlight/data/Documents/kaldi/tools/OpenBLAS/install ***configure failed: CUDA 10_0 does not support g++ (g++-8.2.1). You need g++ \u0026lt; 8.0. ***  比如这里提示cuda10只支持g++8.0以下的版本，不需要降级，从源里安装低版本的g++，Archlinux会在安装cuda时自动安装适合版本的g++，Ubuntu等发行版类似，指定用低版本的g++，这里我们用g++-7：\nCXX=g++-7 ./configure --openblas-root=../tools/OpenBLAS/install --cudatk-dir=/opt/cuda/ --shared make depend -j 8 make -j 8  这样就已经编译好训练需要的工具，之后进行在线解码和处理一些数据集需要额外安装一些工具\n"
},
{
	"uri": "https://blog.codist.me/selfhost/commento/",
	"title": "搭建博客评论系统",
	"tags": [],
	"description": "在Debian9上用开源的Commento搭建博客评论系统",
	"content": "在Debian9上用开源的Commento搭建博客评论系统\n安装 yarn：\ncurl -sS https://dl.yarnpkg.com/debian/pubkey.gpg | sudo apt-key add - echo \u0026quot;deb https://dl.yarnpkg.com/debian/ stable main\u0026quot; | sudo tee /etc/apt/sources.list.d/yarn.list sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install yarn  安装dep（需要已经配置好的go环境）：\ngo get -u github.com/golang/dep/cmd/dep export PATH=$PATH:$GOPATH/bin  下载\u0026amp;编译：\nmkdir -p $GOPATH/gitlab.com/commento cd $GOPATH/gitlab.com/commento git clone https://gitlab.com/commento/commento.git make prod  在根目录下新建脚本server.sh：\n#! /bin/bash # 绑定的域名 export COMMENTO_ORIGIN=https://comment.example.com # 绑定的端口，可以用nginx反向代理到这个端口 export COMMENTO_PORT=8002 # 数据库 PostgreSQL 设置 export COMMENTO_POSTGRES=postgres://postgres:password@127.0.0.1:5432/comment?sslmode=disable # 关闭注册，首次需要注册账号，之后可以关闭注册 #export COMMENTO_FORBID_NEW_OWNERS=true # # github oauth，其他类似 # export GITHUB_KEY= export GITHUB_SECRET= ./build/prod/commento  运行：\nchmod +x server.sh ./server.sh  "
},
{
	"uri": "https://blog.codist.me/stl/",
	"title": "C++ STL",
	"tags": [],
	"description": "",
	"content": " C++ STL 常用数据结构 "
},
{
	"uri": "https://blog.codist.me/asr/thchs30/",
	"title": "使用thchs30数据集",
	"tags": ["kaldi", "ASR"],
	"description": "训练thchs30数据集的一些细节，用训练好的模型在线解码",
	"content": " 1.下载数据集 Kaldi中文语音识别公共数据集有：\n 1.aishell：AI SHELL公司开源178小时中文语音语料及基本训练脚本，见kaldi-master/egs/aishell\n 2.gale_mandarin：中文新闻广播数据集(LDC2013S08, LDC2013S08）\n 3.hkust：中文电话数据集(LDC2005S15, LDC2005T32)\n 4.thchs30：清华大学30小时的数据集，可以在http://www.openslr.org/18/ 下载\n  这里采用thchs30，从http://www.openslr.org/18/ 或者参照它的README下载三个压缩包：\n data_thchs30.tgz 6.4G Mirrors: China\n test-noise.tgz 1.9G Mirrors: China\n resource.tgz 24M Mirrors: China\n  在egs/thchs30/s5下新建文件夹thchs30-openslr，把三个文件解压在该文件夹下\n这个数据集包含以下内容：\n   数据集 音频时长(h) 句子数 词数     train(训练) 25 10000 198252   dev(开发) 2:14 893 17743   test(测试) 6:15 2495 49085    还有训练好的语言模型word.3gram.lm和phone.3gram.lm以及相应的词典lexicon.txt。\n其中dev的作用是在某些步骤与train进行交叉验证的，如local/nnet/run_dnn.sh同时用到exp/tri4b_ali和exp/tri4b_ali_cv。训练和测试的目标数据也分为两类：word（词）和phone（音素）。\n local/thchs-30_data_prep.sh：主要工作是从$thchs/data_thchs30（下载的数据）三部分分别生成word.txt（词序列），phone.txt（音素序列），text（与word.txt相同），wav.scp（语音），utt2pk（句子与说话人的映射），spk2utt（说话人与句子的映射）\n #produce MFCC features：提取MFCC特征，分为两步，先通过steps/make_mfcc.sh提取MFCC特征，再通过steps/compute_cmvn_stats.sh计算倒谱均值和方差归一化。\n #prepare language stuff：构建一个包含训练和解码用到的词的词典。而语言模型已经由王东老师处理好了，如果不打算改语言模型，这段代码也不需要修改。\n a)基于词的语言模型包含48k基于三元词的词，从gigaword语料库中随机选择文本信息进行训练得到，训练文本包含772000个句子，总计1800万词，1.15亿汉字 b)基于音素的语言模型包含218个基于三元音的中文声调，从只有200万字的样本训练得到，之所以选择这么小的样本是因为在模型中尽可能少地保留语言信息，可以使得到的性能更直接地反映声学模型的质量。 c)这两个语言模型都是由SRILM工具训练得到。   2.训练 2.1修改训练脚本 1.首先修改s5/cmd.sh脚本，把原脚本注释掉，修改为本地运行：\n#export train_cmd=queue.pl #export decode_cmd=\u0026quot;queue.pl --mem 4G\u0026quot; #export mkgraph_cmd=\u0026quot;queue.pl --mem 8G\u0026quot; #export cuda_cmd=\u0026quot;queue.pl --gpu 1\u0026quot; export train_cmd=run.pl export decode_cmd=\u0026quot;run.pl --mem 4G\u0026quot; export mkgraph_cmd=\u0026quot;run.pl --mem 8G\u0026quot; export cuda_cmd=\u0026quot;run.pl --gpu 1\u0026quot;  2.然后修改s5/run.sh脚本，需要修改两个地方：\n第一个地方是修改并行任务的数量，cpu核心数*2：\nn=8 #parallel jobs  第二个地方是修改数据集放的位置，修改为上面解压出的目录路径：\n#corpus and trans directory #thchs=/nfs/public/materials/data/thchs30-openslr thchs=/home/countstarlight/data/Documents/kaldi/egs/thchs30/s5/thchs30-openslr  2.2运行训练 ./run.sh  分为几个过程：数据准备，monophone单音素训练， tri1三因素训练， trib2进行lda_mllt特征变换，trib3进行sat自然语言适应，trib4做quick，后面就是dnn了，本地不建议跑dnn。\n建议将run.sh的几个过程分成多个脚本文件来跑，每段脚本里并发数量n必须相同，有些任务是是并发的\u0026amp;，也就是当前脚本执行完也会继续进行，这些并发任务同样需要大量内存(5~8G)，建议用系统监视器监视当前内存占用，等内存恢复到正常水平后再进行下一段脚本。\n exp目录是得到的结果，比如tri1，decode_test_word/scoring_kaldi/best_wer是它的错误率，36.15%。tri1下的final.mdl是得到的模型的链接文件，要用的是它链接到的具体文件。graph_word里的words.txt和HCLG.fst，一个是字典，一个是有限状态机。这3个文件用来识别。\n3.在线识别 这里的“在线”是指一句话还没有说完，即还没有将一句话完整的音频传入就开始识别。\n3.1安装PortAudio 1.先修改tools/extras/install_portaudio.sh，取消对jack的依赖：\n#./configure --prefix=`pwd`/install --with-pic ./configure --prefix=`pwd`/install --with-pic --without-jack  2.安装PortAudio：\ncd tools ./extras/install_portaudio.sh  3.编译扩展程序：\ncd src make ext -j 8  生成的文件在src/onlinebin，其中：\n online-wav-gmm-decode-faster ：从wav文件读取 online-gmm-decode-faster：从麦克风输入声音  4.测试一下麦克风是否正常：\narecord -f cd -r 16000 -d 5 test.wav  16位，16khz，录音5秒，保存文件为test.wav。\n3.2建立相关目录 复制官方online demo文件到thchs30目录，并建立目录：\ncp -r egs/voxforge/online_demo egs/thchs30/ cd egs/thchs30/online_demo #audio用于存放测试用的wav文件(16位，16khz) mkdir -p online-data/audio #从wav文件读取需要用到 touch online-data/audio/trans.txt #models用于存放模型文件 mkdir -p online-data/models/tri1  将s5/exp/tri1中训练得到的： * 35.mdl：模型文件，final.mdl链接到这个文件 * graph_word/words.txt：字典 * graph_word/HCLG.fst：有限状态机\n复制到online-data/models/tri1下。\n3.3修改脚本文件 1.编辑online_demo/run.sh，注释掉如下代码（这段是voxforge例子中下载测试语料和识别模型的。我们测试语料自己准备，模型就是tri1了）：\n#if [ ! -s ${data_file}.tar.bz2 ]; then # echo \u0026quot;Downloading test models and data ...\u0026quot; # wget -T 10 -t 3 $data_url; # # if [ ! -s ${data_file}.tar.bz2 ]; then # echo \u0026quot;Download of $data_file has failed!\u0026quot; # exit 1 # fi #fi  2.修改模型路径为tri1：\n#ac_model_type=tri2b_mmi ac_model_type=tri1  3.修改相关文件路径：\n# online-gmm-decode-faster --rt-min=0.5 --rt-max=0.7 --max-active=4000 \\ # --beam=12.0 --acoustic-scale=0.0769 $ac_model/model $ac_model/HCLG.fst \\ # $ac_model/words.txt '1:2:3:4:5' $trans_matrix;; online-gmm-decode-faster --rt-min=0.5 --rt-max=0.7 --max-active=4000 \\ --beam=12.0 --acoustic-scale=0.0769 $ac_model/35.mdl $ac_model/HCLG.fst \\ $ac_model/words.txt '1:2:3:4:5' $trans_matrix;;  # online-wav-gmm-decode-faster --verbose=1 --rt-min=0.8 --rt-max=0.85\\ # --max-active=4000 --beam=12.0 --acoustic-scale=0.0769 \\ # scp:$decode_dir/input.scp $ac_model/model $ac_model/HCLG.fst \\ # $ac_model/words.txt '1:2:3:4:5' ark,t:$decode_dir/trans.txt \\ # ark,t:$decode_dir/ali.txt $trans_matrix;; online-wav-gmm-decode-faster --verbose=1 --rt-min=0.8 --rt-max=0.85\\ --max-active=4000 --beam=12.0 --acoustic-scale=0.0769 \\ scp:$decode_dir/input.scp $ac_model/35.mdl $ac_model/HCLG.fst \\ $ac_model/words.txt '1:2:3:4:5' ark,t:$decode_dir/trans.txt \\ ark,t:$decode_dir/ali.txt $trans_matrix;;  3.4识别 从麦克风识别：\n./run.sh --test-mode live  如果提示portaudio错误，可参考https://blog.csdn.net/u012236368/article/details/71628777\n识别wav文件：\n./run.sh --test-mode simulated  3.5运行其他模型 tri2b（tri3和tri4同理），把s5/exp/tri2b下的12.mat，35.mdl复制到models/tri2b下，再拷贝其他相应的文件（同tri1的思路），所以/tri2目录下包括如下文件：12.mat、35.mdl、HCLG.fst、words.txt。接着修改run.sh：\n# Change this to \u0026quot;tri2a\u0026quot; if you like to test using a ML-trained model #ac_model_type=tri2b_mmi ac_model_type=tri2b  把12.mat引入命令中：\nac_model=${data_file}/models/$ac_model_type #trans_matrix=\u0026quot;\u0026quot; trans_matrix=\u0026quot;$ac_model/12.mat\u0026quot; audio=${data_file}/audio  添加2个参数--left-context=3 --right-context=3：\n# online-gmm-decode-faster --rt-min=0.5 --rt-max=0.7 --max-active=4000 \\ # --beam=12.0 --acoustic-scale=0.0769 $ac_model/model $ac_model/HCLG.fst \\ # $ac_model/words.txt '1:2:3:4:5' $trans_matrix;; online-gmm-decode-faster --rt-min=0.5 --rt-max=0.7 --max-active=4000 \\ --beam=12.0 --acoustic-scale=0.0769 --left-context=3 --right-context=3 $ac_model/35.mdl $ac_model/HCLG.fst \\ $ac_model/words.txt '1:2:3:4:5' $trans_matrix;;  # online-wav-gmm-decode-faster --verbose=1 --rt-min=0.8 --rt-max=0.85\\ # --max-active=4000 --beam=12.0 --acoustic-scale=0.0769 \\ # scp:$decode_dir/input.scp $ac_model/model $ac_model/HCLG.fst \\ # $ac_model/words.txt '1:2:3:4:5' ark,t:$decode_dir/trans.txt \\ # ark,t:$decode_dir/ali.txt $trans_matrix;; online-wav-gmm-decode-faster --verbose=1 --rt-min=0.8 --rt-max=0.85\\ --max-active=4000 --beam=12.0 --acoustic-scale=0.0769 --left-context=3 --right-context=3 \\ scp:$decode_dir/input.scp $ac_model/35.mdl $ac_model/HCLG.fst \\ $ac_model/words.txt '1:2:3:4:5' ark,t:$decode_dir/trans.txt \\ ark,t:$decode_dir/ali.txt $trans_matrix;;  从麦克风识别：\n./run.sh --test-mode live  识别wav文件：\n./run.sh --test-mode simulated  如果要运行dnn，首先要将nnet1转成nnet2。可以参考链接1和链接2。\n4.算法解读 1.首先用标准的13维MFCC加上一阶和二阶导数训练单音素GMM系统，采用倒谱均值归一化（CMN）来降低通道效应。然后基于具有由LDA和MLLT变换的特征的单音系统构造三音GMM系统，最后的GMM系统用于为随后的DNN训练生成状态对齐。\n2.基于GMM系统提供的对齐来训练DNN系统，特征是40维FBank，并且相邻的帧由11帧窗口（每侧5个窗口）连接。连接的特征被LDA转换，其中维度降低到200。然后应用全局均值和方差归一化以获得DNN输入。DNN架构由4个隐藏层组成，每个层由1200个单元组成，输出层由3386个单元组成。 基线DNN模型用交叉熵的标准训练。 使用随机梯度下降（SGD）算法来执行优化。 将迷你批量大小设定为256，初始学习率设定为0.008。\n3.被噪声干扰的语音可以使用基于深度自动编码器（DAE）的噪声消除方法。DAE是自动编码器（AE）的一种特殊实现，通过在模型训练中对输入特征引入随机破坏。已经表明，该模型学习低维度特征的能力非常强大，并且可以用于恢复被噪声破坏的信号。在实践中，DAE被用作前端管道的特定组件。输入是11维Fbank特征（在均值归一化之后），输出是对应于中心帧的噪声消除特征。然后对输出进行LDA变换，提取全局标准化的常规Fbank特征，然后送到DNN声学模型（用纯净语音进行训练）。\n训练与解码脚本解读\n本节结合官方文档对主要脚本进行解读。 以下流程中的符号解释：-\u0026gt;表示下一步，{}表示循环，[]表示括号内每一个都要进行一次，()表示不同分支下可能进行的操作 1.train_mono.sh 用来训练单音子隐马尔科夫模型，一共进行40次迭代，每两次迭代进行一次对齐操作\ngmm-init-mono-\u0026gt;compile-train-graphs-\u0026gt;align-equal-compiled-\u0026gt;gmm-est-\u0026gt; {gmm-align-compiled-\u0026gt;gmm-acc-stats-ali-\u0026gt;gmm-est}40-\u0026gt;analyze_alignments.sh\n2.train_deltas.sh 用来训练与上下文相关的三音子模型\ncheck_phones_compatible.sh-\u0026gt;acc-tree-stats-\u0026gt;sum-tree-stats-\u0026gt;cluster-phones-\u0026gt;compile-questions-\u0026gt; build-tree-\u0026gt;gmm-init-model-\u0026gt;gmm-mixup-\u0026gt;convert-ali-\u0026gt;compile-train-graphs-\u0026gt; {gmm-align-compiled-\u0026gt;gmm-acc-stats-ali-\u0026gt;gmm-est}35-\u0026gt;analyze_alignments.sh\n3.train_lda_mllt.sh 用来进行线性判别分析和最大似然线性转换\ncheck_phones_compatible.sh-\u0026gt;split_data.sh-\u0026gt;ali-to-post-\u0026gt;est-lda-\u0026gt;acc-tree-stats-\u0026gt;sum-tree-stats-\u0026gt; cluster-phones-\u0026gt;compile-questions-\u0026gt;build-tree-\u0026gt;gmm-init-model-\u0026gt;convert-ali-\u0026gt;compile-train-graphs-\u0026gt; {gmm-align-compiled-\u0026gt;gmm-acc-stats-ali-\u0026gt;gmm-est}35-\u0026gt;analyze_alignments.sh\n4.train_sat.sh 用来训练发音人自适应，基于特征空间最大似然线性回归\ncheck_phones_compatible.sh-\u0026gt;ali-to-post-\u0026gt;acc-tree-stats-\u0026gt;sum-tree-stats-\u0026gt;cluster-phones-\u0026gt;compile-questions-\u0026gt; build-tree-\u0026gt;gmm-init-model-\u0026gt;gmm-mixup-\u0026gt;convert-ali-\u0026gt;compile-train-graphs-\u0026gt; {gmm-align-compiled-\u0026gt;(ali-to-post-\u0026gt;)gmm-acc-stats-ali-\u0026gt;gmm-est}35-\u0026gt;ali-to-post-\u0026gt; gmm-est-\u0026gt;analyze_alignments.sh\n5.train_quick.sh 用来在现有特征上训练模型。 对于当前模型中在树构建之后的每个状态，它基于树统计中的计数的重叠判断的相似性来选择旧模型中最接近的状态。\ncheck_phones_compatible.sh-\u0026gt;ali-to-post-\u0026gt;est-lda-\u0026gt;acc-tree-stats-\u0026gt;sum-tree-stats-\u0026gt; cluster-phones-\u0026gt;compile-questions-\u0026gt;build-tree-\u0026gt;gmm-init-model-\u0026gt;convert-ali-\u0026gt;compile-train-graphs-\u0026gt; {gmm-align-compiled-\u0026gt;gmm-acc-stats-ali-\u0026gt;gmm-est}20-\u0026gt;analyze_alignments.sh\n6.run_dnn.sh 用来训练DNN，包括xent和MPE，\n{make_fbank.sh-\u0026gt;compute_cmvn_stats.sh}[train,dev,test]-\u0026gt;train.sh-\u0026gt;{decode.sh}[phone,word]-\u0026gt; align.sh-\u0026gt;make_denlats.sh-\u0026gt;train_mpe.sh-\u0026gt;{{decode.sh}[phone,word]}3\n7.train_mpe.sh 用来训练dnn的序列辨别MEP/sMBR。 这个阶段训练神经网络以联合优化整个句子，这比帧级训练更接近于一般ASR目标。 sMBR的目的是最大化从参考转录对齐导出的状态标签的期望正确率，而使用网格框架来表示竞争假设。 训练使用每句迭代的随机梯度下降法。 首先使用固定的低学习率1e-5（sigmoids）运行3-5轮。 在第一轮迭代后重新生成词图，我们观察到快速收敛。 我们支持MMI, BMMI, MPE 和sMBR训练。所有的技术在Switchboard 100h集上是相同的，仅仅在sMBR好一点点。 在sMBR优化中，我们在计算近似正确率的时候忽略了静音帧。\n{nnet-train-mpe-sequential}3-\u0026gt;make_priors.sh\n8.train_dae.sh 用来实验基于dae的去噪效果\ncompute_cmvn_stats.sh-\u0026gt;{add-noise-mod.py-\u0026gt;make_fbank.sh-\u0026gt;compute_cmvn_stats.sh}[train,dev,test]-\u0026gt; train.sh-\u0026gt;nnet-concat-\u0026gt;{{decode.sh}[phone,word]}[train,dev,test]\n9.train.sh 用来训练深度神经网络模型，帧交叉熵训练，该相位训练将帧分类为三音状态的DNN。这是通过小批量随机梯度下降完成的。 默认使用Sigmoid隐藏单元，Softmax输出单元和完全连接的AffineTransform层，学习率是0.008，小批量的大小为256。 我们没有使用动量或正则化（注：最佳学习率和隐藏单元的类型不同，sigmoid的值为0.008,tanh为0.00001。 通过‘–feature-transform’和‘-dbn’将input——transform和预训练的DBN传入此脚本，只有输出层被随机初始化。 我们使用提前停止来防止过度拟合，为此我们测量交叉验证集合（即保持集合）上的目标函数， 因此需要两对特征对齐dir来执行监督训练\nfeat-to-dim-\u0026gt;nnet-initialize-\u0026gt;compute-cmvn-stats-\u0026gt;nnet-forward-\u0026gt;nnet-concat-\u0026gt;cmvn-to-nnet-\u0026gt; feat-to-dim-\u0026gt;apply-cmvn-\u0026gt;nnet-forward-\u0026gt;nnet-initialize-\u0026gt;train_scheduler.sh\n10.train_scheduler.sh 典型的情况就是，train_scheduler.sh被train.sh调用。 一开始需要在交叉验证集上运行，主函数需要根据$iter来控制迭代次数和学习率。 学习率会随着目标函数相对性的提高而变化： 如果提高大于’start_halving_impr=0.01’，初始化学习率保持常数 否则学习率在每次迭代中乘以’halving_factor=0.5’来缩小 最后，如果提高小于’end_halving_impr=0.001’，训练终止。\n11.mkgraph.sh 用来建立一个完全的识别网络 12.decode.sh 用来解码并生成词错率结果 13.align_si.sh 对制定的数据进行对齐，作为新模型的输入 14.make_fmllr_feats.sh 用来保存FMLLR特征 15.pretrain_dbn.sh 深度神经网络预训练脚本 16.decode_fmllr.sh 对发音人自适应的模型进行解码操作 17.nnet-train-frmshuff.cc 最普遍使用的神经网络训练工具，执行一次迭代训练。过程： –feature-transform 即时特征扩展 NN输入-目标对的每帧重排 小批量随机梯度下降（SGD）训练 支持的每帧目标函数（选项 - 对象函数）： Xent：每帧交叉熵 Mse：每帧均方误差 18.nnet-forward.cc 通过神经网络转发数据，默认使用CPU。选项： –apply-log :产生神经网络的对数输出(比如：得到对数后验概率) –no-softmax :从模型中去掉soft-max层 —class-frame-counts：从声学得分中减去计算对数的计数\n专有缩写中文解释\ncmvn：倒谱均值和方差归一化 fft：快速傅里叶变换 GMM：高斯混合模型 MFCC：梅尔倒谱系数 pcm：脉冲编码调制 pdf：概率分布函数 PLP：感知线性预测系数 SGMM：子空间高斯混合模型 UBM：通用背景模型 VTLN：特征级声道长度归一化\n"
},
{
	"uri": "https://blog.codist.me/asr/aishell/",
	"title": "使用aishell数据集",
	"tags": ["kaldi", "ASR"],
	"description": "手动获取和处理aishell数据集，训练以及在线识别",
	"content": " 1.安装依赖 安装train_lm.sh：\ncd tools ./extras/install_kaldi_lm.sh  2.获取数据集 和thchs30类似，参照egs/aishell/README.txt，手动下载数据集或运行s5/run.sh会自动下载并解压缩数据集，这里只演示手动下载数据集。\n访问http://www.openslr.org/33/ ，下载两个压缩包：\ndata_aishell.tgz [15G] ( speech data and transcripts ) Mirrors: China\nresource_aishell.tgz [1.2M] ( supplementary resources, incl. lexicon, speaker info ) Mirrors: China\n下载解压缩到一个目录，这里解压缩到aishell-openslr\naishell-openslr/data_aishell/wav里的压缩包需要都解压出来，创建脚本local/untar.sh：\n#!/bin/bash remove_archive=false if [ \u0026quot;$1\u0026quot; == --remove-archive ]; then remove_archive=true shift fi if [ $# -ne 3 ]; then echo \u0026quot;Usage: $0 \u0026lt;data-base\u0026gt;\u0026quot; echo \u0026quot;e.g.: $0 /export/a05/xna/data\u0026quot; fi data=$1 part=\u0026quot;data_aishell\u0026quot; if [ ! -d \u0026quot;$data\u0026quot; ]; then echo \u0026quot;$0: no such directory $data\u0026quot; exit 1; fi if [ -f $data/$part/.complete ]; then echo \u0026quot;$0: data part $part was already successfully extracted, nothing to do.\u0026quot; exit 0; fi touch $data/$part/.complete if [ $part == \u0026quot;data_aishell\u0026quot; ]; then cd $data/$part/wav for wav in ./*.tar.gz; do echo \u0026quot;Extracting wav from $wav\u0026quot; tar -zxf $wav \u0026amp;\u0026amp; rm $wav done fi echo \u0026quot;$0: Successfully downloaded and un-tarred $data/$part.tgz\u0026quot; exit 0;  修改训练脚本run.sh：\n修改数据集存放路径\n#data=/export/a05/xna/data data=aishell-openslr #替换为你解压缩出数据集的路径 data_url=www.openslr.org/resources/33  修改下载解压缩数据集为我们新建的解压缩脚本\n#local/download_and_untar.sh $data $data_url data_aishell || exit 1; #local/download_and_untar.sh $data $data_url resource_aishell || exit 1; local/untar.sh $data || exit 1;  这样在首次运行训练脚本时会执行我们的解压缩脚本，对wav文件进行批量解压缩\n3.训练 3.1修改训练脚本 修改cmd.sh：\n#export train_cmd=\u0026quot;queue.pl --mem 2G\u0026quot; #export decode_cmd=\u0026quot;queue.pl --mem 4G\u0026quot; #export mkgraph_cmd=\u0026quot;queue.pl --mem 8G\u0026quot; export train_cmd=run.pl export decode_cmd=\u0026quot;run.pl --mem 4G\u0026quot; export mkgraph_cmd=\u0026quot;run.pl --mem 8G\u0026quot;  和thchs30一样，建议对训练脚本run.sh分成多个文件，run1.sh\u0026hellip;，分次执行，公共的内容是：\n#data=/export/a05/xna/data data=aishell-openslr #替换为你解压缩出数据集的路径 data_url=www.openslr.org/resources/33 . ./cmd.sh  3.2训练 ./run.sh # run1.sh run2.sh ...  4.识别\u0026ndash;在线解码 以aishell的chain模型为例\n4.1生成配置文件 所有操作都在aishell/s5下\n4.1.1使用nnet3模型 1.建立需要的软链接：\ncd exp/nnet3/tdnn_sp ln -s 0.mdl final.mdl  2.生成配置文件\n./steps/online/nnet3/prepare_online_decoding.sh \\ --add-pitch true \\ --mfcc-config conf/mfcc_hires.conf \\ data/lang \\ exp/nnet3/extractor \\ exp/nnet3/tdnn_sp \\ exp/nnet3/nnet_online   --add-pitch：aishell的nnet3和chain模型输入的特征在MFCC的基础上还加入了pitch特征，反应了音高的信息\n --mfcc-config：mfcc使用的配置文件，注意这里我们使用的是mfcc_hires.conf\n data/lang：nnet3模型解码网络图中G.fst和L.fst文件以及词汇表words.txt文件\n exp/nnet3/tdnn_sp：nnet3模型\n exp/nnet3/nnet_online：生成的配置文件存放的目录，自己定义\n  4.1.2使用chain模型(未经测试) ./steps/online/nnet3/prepare_online_decoding.sh \\ --add-pitch true \\ --mfcc-config conf/mfcc_hires.conf \\ data/lang \\ exp/nnet3/extractor \\ exp/chain/tdnn_1a_sp \\ exp/chain/nnet_online   --add-pitch：aishell的nnet3和chain模型输入的特征在MFCC的基础上还加入了pitch特征，反应了音高的信息\n --mfcc-config：mfcc使用的配置文件，注意这里我们使用的是mfcc_hires.conf\n data/lang_chain：存储chain模型解码网络图中G.fst和L.fst文件以及词汇表words.txt文件\n exp/chain/tdnn_1a_sp：存储chain模型\n exp/chain/nnet_online：生成的配置文件存放的目录，自己定义\n  4.2解码 4.2.1使用nnet3模型(未经测试) online2-wav-nnet3-latgen-faster \\ --config=exp/nnet3/nnet_online/conf/online.conf \\ --do-endpointing=false \\ --frames-per-chunk=20 \\ --extra-left-context-initial=0 \\ --online=true \\ --frame-subsampling-factor=3 \\ --min-active=200 \\ --max-active=7000 \\ --beam=15.0 \\ --lattice-beam=6.0 \\ --acoustic-scale=1.0 \\ --word-symbol-table=s5/data/lang/words.txt \\ s5/exp/chain/tdnn_1a_sp/final.mdl \\ s5/exp/chain/tdnn_1a_sp/graph/HCLG.fst \\ ark:s5/data/test/spk2utt \\ scp:s5/data/test/wav.scp \\ ark,t:20190308.txt  4.2.2使用chain模型(未经测试) online2-wav-nnet3-latgen-faster \\ --config=s5/exp/chain/nnet_online/conf/online.conf \\ --do-endpointing=false \\ --frames-per-chunk=20 \\ --extra-left-context-initial=0 \\ --online=true \\ --frame-subsampling-factor=3 \\ --min-active=200 \\ --max-active=7000 \\ --beam=15.0 \\ --lattice-beam=6.0 \\ --acoustic-scale=1.0 \\ --word-symbol-table=s5/data/lang/words.txt \\ s5/exp/chain/tdnn_1a_sp/final.mdl \\ s5/exp/chain/tdnn_1a_sp/graph/HCLG.fst \\ ark:s5/data/test/spk2utt \\ scp:s5/data/test/wav.scp \\ ark,t:20190308.txt  "
},
{
	"uri": "https://blog.codist.me/embedded/",
	"title": "嵌入式开发",
	"tags": [],
	"description": "",
	"content": " 嵌入式开发 这里是一些做过的嵌入式Linux平台遇到的问题和解决办法\n"
},
{
	"uri": "https://blog.codist.me/asr/cvte/",
	"title": "使用cvte预训练模型",
	"tags": ["kaldi", "ASR"],
	"description": "获取cvte的预训练模型并进行解码测试",
	"content": " cvte开放了已经训练好的模型，不用再花费大量时间和算力去训练，但注意cvte没有开源数据集和模型配置\n获取模型 从 http://kaldi-asr.org/models/m2 下载 0002_cvte_chain_model.tar.gz(3.5G)\n解压缩到kaldi/egs下，注意kaldi/egs/换成安装kaldi对应的目录：\ntar -zxvf 0002_cvte_chain_model.tar.gz -C kaldi/egs/  解压生成目录kaldi/egs/cvte，按照cvte/README.txt链接steps，utils和score.sh，由于需要修改utils中的脚本，这里直接拷贝utils文件夹：\ncd kaldi/egs/cvte/s5/ ln -s ../../wsj/s5/steps steps #ln -s ../../wsj/s5/utils utils cp -r ../../wsj/s5/utils utils cd local/ ln -s ../steps/score_kaldi.sh score.sh  修改解码脚本 修改utils/lang/check_phones_compatible.sh为：\n# check if the files exist or not if [ ! -f $table_first ]; then if [ ! -f $table_second ]; then echo \u0026quot;$0: Error! Both of the two phones-symbol tables are absent.\u0026quot; echo \u0026quot;Please check your command\u0026quot; #exit 1; 这里注释掉 else # The phones-symbol-table1 is absent. The model directory maybe created by old script. # For back compatibility, this script exits silently with status 0. exit 0; fi elif [ ! -f $table_second ]; then # The phones-symbol-table2 is absent. The model directory maybe created by old script. # For back compatibility, this script exits silently with status 0. exit 0; fi  运行解码测试 chmod +x *.sh ./run.sh  需要在大内存平台上，在本地12G内存的计算机上由于内存不够而出错\n"
},
{
	"uri": "https://blog.codist.me/linux/",
	"title": "桌面Linux",
	"tags": [],
	"description": "",
	"content": " 桌面Linux 这是使用Linux桌面发行版时遇到的一些需求\n"
},
{
	"uri": "https://blog.codist.me/android/",
	"title": "Android底层",
	"tags": [],
	"description": "",
	"content": " Android底层 这里是做安卓底层（内核，驱动）遇到的一些问题和解决办法\n"
},
{
	"uri": "https://blog.codist.me/",
	"title": "首页",
	"tags": [],
	"description": "",
	"content": " Codist 这是Codist的博客，是一些在开发中遇到的问题和解决方法的整理\n欢迎补充和错误指正\n 基于kaldi的中文语音识别    安装kaldi   使用thchs30数据集   使用aishell数据集   使用cvte预训练模型    C\u0026#43;\u0026#43; STL    SET容器 - 自定义排序和去重    嵌入式开发    用minicom通过串口发送文件   Qt Creator 配置交叉编译   Linux上通过串口连接嵌入式Linux终端    桌面Linux    Linux上摄像设备的使用    Android底层    Math Sample   Android\u0026#39;s selinux   将Android的系统日志输出到文件    自建代码托管\u0026amp;CI/CD    搭建博客评论系统    Git使用    Git 用GPG签名    "
},
{
	"uri": "https://blog.codist.me/selfhost/",
	"title": "自建代码托管&amp;CI/CD",
	"tags": [],
	"description": "",
	"content": " 自建代码托管\u0026amp;CI/CD 搭建开源的代码托管\u0026amp;CI/CD 以及评论系统\n"
},
{
	"uri": "https://blog.codist.me/git/",
	"title": "Git使用",
	"tags": [],
	"description": "",
	"content": " Git使用 一些不太常用的Git使用细节 "
},
{
	"uri": "https://blog.codist.me/embedded/work6/",
	"title": "用minicom通过串口发送文件",
	"tags": ["Minicom", "Linux"],
	"description": "",
	"content": " 1. 设置文件路径  设置需要传送的文件所在的路径\n 按下组合键 Ctrl + A , 再按下 O\n 选择 Filenames and paths：\n    设置 A-Download directory （将文件从设备传送到本地的路径）\n 设置 B-Upload directory （将文件从本地上传到设备的路径）向设备发送文件时，选择这个目录里的文件。\n 按下 esc 然后选择 Save setup as dfl来保存设置：\n  2. 发送文件到设备  在minicom里（已经连接到设备的终端）\nrx filename  按下组合键 Ctrl + A，然后按下S\n 选择 xmodem\n 选择要发送的文件\n  3. 在设备上运行刚才发送过去的文件（假设是Qt图形界面程序） 在minicom里（已经连接到设备的终端）：\nchmod +x filename #给文件授予执行权限 ./filename #运行  如果设备上的系统没有图形化环境 添加参数 -qws：\n./filename -qws  如果希望旋转屏幕显示 添加参数 -display 以及要旋转的角度（顺时针）：\n./filename -display \u0026quot;Transformed:Rot270\u0026quot;  "
},
{
	"uri": "https://blog.codist.me/embedded/work5/",
	"title": "Qt Creator 配置交叉编译",
	"tags": ["Qt-creator"],
	"description": "",
	"content": " 1. 获取对应的交叉编译工具链 假设目标平台的处理器是 imx28，则相应的工具链为 arm-fsl-linux-gnueabi\n"
},
{
	"uri": "https://blog.codist.me/embedded/work4/",
	"title": "Linux上通过串口连接嵌入式Linux终端",
	"tags": ["Linux", "Serial-USB"],
	"description": "",
	"content": " 1. 安装 minicom Debian/Ubuntu：\nsudo aptitude update sudo aptitude install minicom  2. 插上设备并查看系统是否已经检测到设备 sudo dmesg | grep tty  如果没有检测到设备，得到的结果类似： [ 0.000000] console [tty0] enabled [ 8.264501] systemd[1]: Created slice system-getty.slice.   拔下usb转串口线，输入命令 lsusb会看到一些已经连接到usb的设备：\nBus 002 Device 002: ID 8087:8000 Intel Corp. Bus 002 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub Bus 001 Device 002: ID 8087:8008 Intel Corp. Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub Bus 004 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub Bus 003 Device 004: ID 13d3:5188 IMC Networks Bus 003 Device 006: ID 13d3:3402 IMC Networks Bus 003 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub  再次插上usb转串口线，再次运行命令 lsusb，会看到输出结果相比之前增加了一行：\nBus 002 Device 002: ID 8087:8000 Intel Corp. Bus 002 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub Bus 001 Device 002: ID 8087:8008 Intel Corp. Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub Bus 004 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub Bus 003 Device 004: ID 13d3:5188 IMC Networks Bus 003 Device 006: ID 13d3:3402 IMC Networks Bus 003 Device 008: ID 18f8:0f99 --- --- --- (注意这行是新加的！) Bus 003 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub  现在我们知道了usb转串口的 vendor id 和 product id，让我们加载Linux内核的usbserial模块来激活这个设备：\nsudo modprobe usbserial vendor=0x18f8 product=0x0f99  再次运行 dmesg 命令，输出结果类似：\nusbserial_generic 1-1:1.0: generic converter detected usb 1-1: generic converter now attached to ttyUSB0 usbcore: registered new interface driver usbserial_generic   可以把自动加载usbserial模块添加到开机启动里，编辑文件/etc/modules，添加一行：\n usbserial vendor=0x18f8 product=0x0f99  3. 连接到设备 假设设备路径为 /dev/ttyUSB0，运行命令:\nsudo minicom -s  在Serial port setup里修改第一行为/dev/ttyUSB0\n选择Save setup as dfl保存设置\n"
},
{
	"uri": "https://blog.codist.me/linux/work3/",
	"title": "Linux上摄像设备的使用",
	"tags": [],
	"description": "",
	"content": " 在Linux上配置网络摄像头，用到的一些软件，以及如何录制和播放设备的视频输入\n1. 查找设备  插上摄像设备（通常是通过USB）\n 列出所有的 video4linux 设备:\nls -ltr /dev/video*  得到的输出类似于：\ncrw-rw----+ 1 root video 81, 0 Nov 11 09:06 /dev/video0   这里的摄像设备名称是 */dev/video0*，如果没有看到任何 /dev/video 文件，查看#排查问题。如果有多个 video4linux 设备，比如是一个tv card，摄像头设备应该显示为 /dev/video1 或类似的。但它的时间（在这个例子中是 Nov 11 09:06）应该是你插上它的时间。\n2.测试设备  如果安装有 vlc，可以启动它，选择 Media -\u0026gt; Open Capture Device -\u0026gt; Video device name = /dev/video0 -\u0026gt; Play\n 如果安装有mplayer，可以使用：\nmplayer tv:// -tv driver=v4l2:width=640:height=480:device=/dev/video0 -fps 30   3.使用设备 3.1录制视频 要捕获设备的视频输入，可以使用 cheese，一个不错的图形界面软件，你可以用它边看边录制设备的视频输入，录制保存的文件格式不太常见(.webm)，但用vlc可以播放。\nYou can also automate video recording so you can capture the camera stream with sitting in front of the computer. To do this you can\n use the software mencoder:\nmencoder tv:// -tv driver=v4l2:width=320:height=240:device=/dev/video0 -nosound -ovc lavc -o myvideo.avi  or use the software streamer. Here are two examples:\nstreamer -c /dev/video0 -f jpeg -F stereo -o myvideo.avi -t 0:05   3.2视频通话 视频通话，在Linux上使用skype.\n3.3查看视频输入 查看摄像设备的视频输入，使用cheese 或 mplayer:\nmplayer -fps 30 -cache 128 -tv driver=v4l2:width=640:height=480:device=/dev/video0 tv://  或者用vlc，你可以用root权限运行vlc，来查看你的摄像设备 /dev/video0，启动vlc并选择 Media -\u0026gt; Open Capture Device -\u0026gt; Video device name = /dev/video0 -\u0026gt; Play\n排查问题 Troubleshooting heavily depends on the distribution and version you are using. If you have done cabling correctly and a device file /dev/video* does not appear, your kernel probably does not know the hardware. In this case you may have to install the device driver separately because it may not be part of the kernel.\nSUSE Linux 11.0 and earlier This has been tested with SUSE Linux 11.0 x64 but should work with any earlier SUSE version. You will need to log in as user root. To find out what driver you need, open a console and call\nhwinfo --usb  If a Logitech Quickcam Messenger is plugged in the answer will be like:\n06: USB 00.2: 0000 Unclassified device [Created at usb.122] UDI: /org/freedesktop/Hal/devices/usb_device_46d_8da_noserial_if2 Unique ID: Eopr.vE+cdFBwClB Parent ID: uIhY.uOe2OKugI8D SysFS ID: /devices/pci0000:00/0000:00:1a.2/usb3/3-1/3-1:1.2 SysFS BusID: 3-1:1.2 Hardware Class: unknown Model: \u0026quot;Logitech QuickCam Messanger\u0026quot; Hotplug: USB Vendor: usb 0x046d \u0026quot;Logitech, Inc.\u0026quot; Device: usb 0x08da \u0026quot;QuickCam Messanger\u0026quot; Revision: \u0026quot;1.00\u0026quot; Driver: \u0026quot;snd-usb-audio\u0026quot; Driver Modules: \u0026quot;snd_usb_audio\u0026quot; Speed: 12 Mbps Module Alias: \u0026quot;usb:v046Dp08DAd0100dc00dsc00dp00ic01isc02ip00\u0026quot; Driver Info #0: Driver Status: quickcam_messenger is active Driver Activation Cmd: \u0026quot;modprobe quickcam_messenger\u0026quot; Driver Info #1: Driver Status: gspca is active Driver Activation Cmd: \u0026quot;modprobe gspca\u0026quot; Config Status: cfg=new, avail=yes, need=no, active=unknown Attached to: #20 (Hub)  This means you can install and load the webcam driver like this:\nyast -i gspcav-kmp-default modprobe gspca  Now you should see a video device:\nls /dev/video* /dev/video /dev/video0  That means you can install and start your webcam-viewer-software. We choose gqcam:\nyast -i gqcam gqcam  It works. You see a video what from what is going on in front of your webcam.\nUbuntu This has been tested with Ubuntu 8.10 x32 but should work with any Ubuntu version. Find out the driver activation command of your webcam. For this, first install the software hwinfo. Open a consoleand type:\nsudo apt-get install hwinfo  Then call hwinfo:\nhwinfo --usb  If a Logitech Quickcam Messenger is plugged in the response will be like:\n04: USB 00.2: 0000 Unclassified device [Created at usb.122] UDI: /org/freedesktop/Hal/devices/usb_device_46d_8da_noserial_if2 Unique ID: 4ajv.vE+cdFBwClB Parent ID: k4bc._Mkd+LmXb03 SysFS ID: /devices/pci0000:00/0000:00:11.0/0000:02:00.0/usb1/1-1/1-1:1.2 SysFS BusID: 1-1:1.2 Hardware Class: unknown Model: \u0026quot;Logitech QuickCam Messanger\u0026quot; Hotplug: USB Vendor: usb 0x046d \u0026quot;Logitech, Inc.\u0026quot; Device: usb 0x08da \u0026quot;QuickCam Messanger\u0026quot; Revision: \u0026quot;1.00\u0026quot; Driver: \u0026quot;snd-usb-audio\u0026quot; Driver Modules: \u0026quot;snd_usb_audio\u0026quot; Speed: 12 Mbps Module Alias: \u0026quot;usb:v046Dp08DAd0100dc00dsc00dp00ic01isc02ip00\u0026quot; Driver Info #0: Driver Status: gspca_zc3xx is active Driver Activation Cmd: \u0026quot;modprobe gspca_zc3xx\u0026quot; Config Status: cfg=new, avail=yes, need=no, active=unknown Attached to: #8 (Hub)  Activate the driver:\nsudo modprobe gspca_zc3xx  Now you should be able to see the video device:\nls /dev/video* /dev/video0  You can now test your webcam using the software cheese:\nsudo apt-get install cheese cheese  Other webcams If you have another webcam, try the above nevertheless. If it does not work, exchange the driver gspca against uvcvideo:\nyast -i uvcvideo_kmp_default modprobe uvcvideo  and start gqcam again.\nTestbed The following webcams have been found working with this tutorial:\n Logitech Quickcam messenger Philips Webcam SPC220NC  A general list of working webcams can be found at http://mxhaard.free.fr/spca5xx.html.\nThe guide has been tested with SUSE Linux 11.4 till 13.2 and Ubuntu.\nSee also  hardware http://en.opensuse.org/Webcam http://www.linux.com/feature/126186 http://ubuntulinuxhelp.com/linux-driver-for-quickcam-usb-cameras-logitech-quickcam-fusion/ http://www.goldmann.de/webcam-linux_tipp_408.html http://wiki.ubuntuusers.de/Webcam  "
},
{
	"uri": "https://blog.codist.me/android/work8/",
	"title": "Math Sample",
	"tags": ["example", "math"],
	"description": "",
	"content": "KaTeX can be used to generate complex math formulas server-side.\n$$ \\phi = \\frac{(1+\\sqrt{5})}{2} = 1.6180339887\\cdots $$\nAdditional details can be found on GitHub or on the Wiki.\nExample 1 If the text between $$ contains newlines it will rendered in display mode:\n$$ f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)\\,e^{2 \\pi i \\xi x}\\,d\\xi $$  $$ f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)\\,e^{2 \\pi i \\xi x}\\,d\\xi $$\nExample 2 $$ \\frac{1}{\\Bigl(\\sqrt{\\phi \\sqrt{5}}-\\phi\\Bigr) e^{\\frac25 \\pi}} = 1+\\frac{e^{-2\\pi}} {1+\\frac{e^{-4\\pi}} {1+\\frac{e^{-6\\pi}} {1+\\frac{e^{-8\\pi}} {1+\\cdots} } } } $$  ​​$$ \\frac{1}{\\Bigl(\\sqrt{\\phi \\sqrt{5}}-\\phi\\Bigr) e^{\\frac25 \\pi}} = 1+\\frac{e^{-2\\pi}} {1+\\frac{e^{-4\\pi}} {1+\\frac{e^{-6\\pi}} {1+\\frac{e^{-8\\pi}} {1+\\cdots} } } } $$ ​​\nExample 3 $$ 1 + \\frac{q^2}{(1-q)}+\\frac{q^6}{(1-q)(1-q^2)}+\\cdots = \\prod_{j=0}^{\\infty}\\frac{1}{(1-q^{5j+2})(1-q^{5j+3})}, \\quad\\quad \\text{for }\\lvert q\\rvert\u0026lt;1. $$  $$ 1 + \\frac{q^2}{(1-q)}+\\frac{q^6}{(1-q)(1-q^2)}+\\cdots = \\prod_{j=0}^{\\infty}\\frac{1}{(1-q^{5j+2})(1-q^{5j+3})}, \\quad\\quad \\text{for }\\lvert q\\rvert\u0026lt;1. $$\n"
},
{
	"uri": "https://blog.codist.me/android/work2/",
	"title": "Android&#39;s selinux",
	"tags": ["Android", "selinux"],
	"description": "",
	"content": "一、适用情景 当在init.rc中新增service：\nservice ro_isn /system/bin/isn.sh class late_start user root Oneshot\nkernel log会打印以下log：\nWarning! Service ro_isn needs a SELinux domain defined; please fix!\n这是因为Service ro_isn没有在SELinux的监控之下，这种情况会提示你定义一个SELinux。\n在这种情况下，你可以：\n1.无视该条log，Service功能不受影响。各种权限不受限制。但是这样做会有风险。\n2.为Service ro_isn定义一个SELinux domain，仅添加需要的权限，未允许的权限操作会被拒绝。具体方法请参照下节。\n二、解决方法 1.在devices/…/sepolicy/目录下新增ro_isn.te文件，内容如下：\ntype ro_isn, domain; type ro_isn_exec, exec_type, file_type;  2.在devices/…/sepolicy/Android.mk中添加ro_isn.te文件，内容如下：\nBOARD_SEPOLICY_UNION := ... hostapd.te ro_isn.te  3.在devices/…/sepolicy/file_contexts中增加如下内容：\n################################### #System files # ... /system/vendor/bin/slim_ap_daemon u:object_r:location_exec:s0 /system/bin/isn.sh u:object_r:ro_isn_exec:s0  4.在device/\u0026hellip;/sepolicy/service_contexts中添加：\nservicename u:object_r:ro_isn_service:s0  5.在init.rc中service ro_isn下添加:\nsecure context by seclabel service ro_isn /system/bin/isn.sh class late_start user root oneshot seclabel u:r:ro_isn:s0  6.编译并烧录bootimage\n 如果编译不成功，失败原因如下：\nError while expanding policy libsepol.check_assertion_helper: neverallow on line 233 of external/sepolicy/domain.te (or line 5194 of policy.conf) violated by allow ro_isn system_file:file { entrypoint }; make: *** [out/target/product/msm8226/obj/ETC/sepolicy_intermediates/sepolicy] 错误 1   这是因为系统在domain.te中定义了全局的neverallow策略，与ro_isn.te中allow的策略有冲突：\nallow ro_isn system_file:file { entrypoint }; neverallow domain { file_type -exec_type }:file entrypoint;  请确定自己的service有必要需要这个权限。如无必要，请在自己的code中删除掉相关操作；如必要，可以在external/sepolicy/domain.te中冲突的neverallow语句中添加自己为例外：\nneverallow { domain -ro_isn } { file_type -exec_type }:file entrypoint;   在service ro_isn运行时，出现关于“ro_isn”的avc: denied log\n\u0026lt;6\u0026gt;[ 13.547188](CPU:0-pid:320:logd.auditd) type=1400 audit(17468992.410:7): avc: denied { entrypoint } for pid=272 comm=\u0026quot;init\u0026quot; path=\u0026quot;/system/bin/isn.sh\u0026quot; dev=\u0026quot;mmcblk0p38\u0026quot; ino=631 scontext=u:r:ro_isn:s0 tcontext=u:object_r:system_file:s0 tclass=file   a.按照如下规则在ro_isn.te添加权限\nSELinux规则语句一般如下： allow A B:C D; 可以从log中分别获取ABCD四个参数。\n比如这行warning log：\navc: denied { entrypoint } for pid=272 comm=\u0026quot;init\u0026quot; path=\u0026quot;/system/bin/isn.sh\u0026quot; dev=\u0026quot;mmcblk0p38\u0026quot; ino=631 scontext=u:r:ro_isn:s0 tcontext=u:object_r:system_file:s0 tclass=file  那么我们就得出最后的规则是：\nallow qcomsysd block_device:dir { search }; allow ro_isn system_file:file { entrypoint };  重复该步骤,直到没有关于“ro_isn”的avc: denied log\n"
},
{
	"uri": "https://blog.codist.me/android/work1/",
	"title": "将Android的系统日志输出到文件",
	"tags": ["Android", "log"],
	"description": "",
	"content": " 首先，在产品目录的init.XXX.rc文件中，添加相应的service，\n#start log service start logd on property:service.logcat.enable=1 start logcat_service on property:service.logcat.enable=0 stop logcat_service #log services service logcat_service /system/bin/logcat -b system -b events -b main -b radio -k -n 10 -v threadtime -r5000 -f /data/Logs/Log.0/logcat.log user root group log system class main disabled service logd /system/bin/sh /system/bin/logd.sh user system group log oneshot  然后，在目标平台的system/bin下添加脚本文件logd.sh，处理存储的log日志，以及设置属性，开启logcat_service,\n#!/system/bin/sh # #Global folder \u0026amp; cmd params # OUTPUT_DIR=/data LOG=Logs index=2 LOG_DIR[0]=$OUTPUT_DIR/$LOG/Log.0 LOG_DIR[1]=$OUTPUT_DIR/$LOG/Log.1 LOG_DIR[2]=$OUTPUT_DIR/$LOG/Log.2 RM=rm MV=\u0026quot;mv\u0026quot; MKDIR=mkdir UMASK=umask #set default permission 0775 $UMASK 002 #Init the three folders i=0 while [ \u0026quot;$i\u0026quot; -le \u0026quot;$index\u0026quot; ] do $MKDIR -p ${LOG_DIR[$i]} i=$(($i+1)) done #Transfer the three folders ((i=$index-1)) $RM -r ${LOG_DIR[$index]}/* while [ \u0026quot;$i\u0026quot; -ge \u0026quot;0\u0026quot; ] do $MV ${LOG_DIR[$i]}/* ${LOG_DIR[$i+1]} i=$(($i-1)) done $RM -r ${LOG_DIR[0]}/* #start logcat service setprop service.logcat.enable 1 mkdir /data/www cp -R /system/var/www/ /data/ ln -s /storage/external/ /data/www/sdcard  "
},
{
	"uri": "https://blog.codist.me/tags/android/",
	"title": "Android",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.codist.me/tags/asr/",
	"title": "Asr",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.codist.me/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.codist.me/tags/example/",
	"title": "Example",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.codist.me/tags/kaldi/",
	"title": "Kaldi",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.codist.me/tags/linux/",
	"title": "Linux",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.codist.me/tags/log/",
	"title": "Log",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.codist.me/tags/math/",
	"title": "Math",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.codist.me/tags/minicom/",
	"title": "Minicom",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.codist.me/tags/qt-creator/",
	"title": "Qt Creator",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.codist.me/tags/selinux/",
	"title": "Selinux",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.codist.me/tags/serial-usb/",
	"title": "Serial Usb",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://blog.codist.me/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]