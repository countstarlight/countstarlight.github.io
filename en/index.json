[{"authors":["admin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://blog.codist.me/en/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/authors/admin/","section":"authors","summary":"","tags":null,"title":"Codist","type":"authors"},{"authors":null,"categories":null,"content":"这是Codist的博客，是一些在开发中遇到的问题和解决方法的整理\n欢迎补充和错误指正\n","date":1461081600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1504368000,"objectID":"4cdd37113783e47641dd300543c94e1b","permalink":"https://blog.codist.me/en/docs/","publishdate":"2016-04-20T00:00:00+08:00","relpermalink":"/en/docs/","section":"docs","summary":"这是Codist的博客，是一些在开发中遇到的问题和解决方法的整理 欢迎补充和错误指正","tags":null,"title":"Codist","type":"docs"},{"authors":null,"categories":null,"content":" Homo 是一个高性能，易于扩展且完全开源的自然交互系统\n第一步 安装 - 安装Homo到你的平台\n获取数据 - 获取Homo运行所需的数据\n配置 - 详细了解Homo的配置项\n运行 - 使用不同模式启动Homo\n进阶 ","date":1560355200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1560441600,"objectID":"5c195026e5234d446b7eb942245f1159","permalink":"https://blog.codist.me/en/homo/","publishdate":"2019-06-13T00:00:00+08:00","relpermalink":"/en/homo/","section":"homo","summary":"Homo 是一个高性能，易于扩展且完全开源的自然交互系统 第一步 安装 - 安装Homo到你的平台 获取数据 - 获取Homo运行所需的数据 配置 - 详细了解Homo","tags":null,"title":"概述","type":"docs"},{"authors":null,"categories":null,"content":"","date":1560355200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560355200,"objectID":"61b7a2f1e6969d61132024abf74d0bc9","permalink":"https://blog.codist.me/en/homo/wake-up/","publishdate":"2019-06-13T00:00:00+08:00","relpermalink":"/en/homo/wake-up/","section":"homo","summary":"","tags":null,"title":"自定义唤醒词","type":"docs"},{"authors":null,"categories":null,"content":" 首先获取Homo源码:\ngit clone https://github.com/countstarlight/homo.git  开始安装依赖\n1.安装依赖 1.1 系统依赖 1.1.1 PortAudio 和 SWIG Homo 使用 PortAudio 从音频硬件读取和处理音频，这个库通常已经被安装\nsphinxbase 依赖 swig\nDebian / Ubuntu:\nsudo apt-get install portaudio19-dev swig  Arch Linux / Manjaro Linux:\n# 需要安装 pulseaudio-alsa 使得和 pulseaudio 一起工作时不会造成崩溃 sudo pacman -S portaudio pulseaudio-alsa swig  1.1.2 WebView Homo的图形界面实现\nDebian / Ubuntu:\nsudo apt-get install libwebkit2gtk-4.0-dev  Arch Linux / Manjaro Linux:\nsudo pacman -S webkit2gtk  1.1.3 PocketSphinx Homo 借助 PocketSphinx 实现离线唤醒，控制语音录制开始和结束的时机\na) 安装 sphinxbase\nPocketSphinx 依赖 sphinxbase\nLinux / Unix:\ngit clone https://github.com/cmusphinx/sphinxbase.git cd sphinxbase ./autogen.sh ./configure make -j 4 sudo make install  b) 安装 PocketSphinx\nLinux / Unix:\ngit clone https://github.com/cmusphinx/pocketsphinx.git cd pocketsphinx ./autogen.sh ./configure sudo make install  1.2 Python依赖 推荐使用 virtualenv 为每个项目创建一个独立的python环境\nPython推荐使用3.6.8版本\n目前自然语言理解和文本情感分析是两个单独的模块，稍后会合并到一起，以简化操作\n1.2.1 安装文本情感分析引擎的依赖 cd homo/sentiment # 创建一个python3.6的环境 # 需要已经安装有pyhton3.6 virtualenv --python=python3.6 env3.6 # 进入创建的环境 source env3.6/bin/activate # 安装依赖 pip install -r requirements.txt # 使用国内镜像 # pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -r requirements.txt  1.2.2 安装自然语言理解引擎的依赖 cd homo/nlu # 创建一个python3.6的环境 # 需要已经安装有pyhton3.6 virtualenv --python=python3.6 env3.6 # 进入创建的环境 source env3.6/bin/activate # 安装依赖 pip install -r requirements.txt # 使用国内镜像 # pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -r requirements.txt  2.编译主程序 这需要你确保已经配置好Go开发环境:\nLinux:\n从 https://golang.org/dl/下载 Linux 对应的.tar.gz文件，假设下载到的是 go1.12.6.linux-amd64.tar.gz:\n# 直接解压缩到 /usr/local sudo tar -C /usr/local -xzf go1.12.6.linux-amd64.tar.gz  编译 homo-webview：\ncd homo # 生成资源文件 make gen # 编译 make webview  生成的文件为：homo-webview\n","date":1560355200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560355200,"objectID":"7978fb6fd34e47d5a2727bc6876ec675","permalink":"https://blog.codist.me/en/homo/install/","publishdate":"2019-06-13T00:00:00+08:00","relpermalink":"/en/homo/install/","section":"homo","summary":"首先获取Homo源码: git clone https://github.com/countstarlight/homo.git 开始安装依赖 1.安装依赖 1.1 系统依赖 1.1.1 PortAudio 和 SWIG Homo 使用 PortAudio 从音频硬件读取和处理音频，这个库通常已经被安装 sphinxbase 依赖 swig Debian / Ubuntu: sudo","tags":null,"title":"安装","type":"docs"},{"authors":null,"categories":null,"content":" std::set，是基于红黑树的平衡二叉树的数据结构实现的一种容器，因为其中所包含的元素的值是唯一的，因此主要用于排序和去重。\n1.使用内置的比较函数less 定义内置类型的set对象，限制：\n 用于比较内置类型，如int，char 只能对一个内置类型进行排序或去重  示例排序(c++11)：\n#include \u0026lt;iostream\u0026gt; #include \u0026lt;set\u0026gt; int main() { std::set\u0026lt;int\u0026gt; testSet; testSet.insert(20); testSet.insert(10); for (auto i : testSet) { std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026quot; \u0026quot;; } std::cout \u0026lt;\u0026lt; std::endl; return 0; }  Microsoft Visual C++ 6.0(c++98)：\n#include \u0026lt;iostream\u0026gt; #include \u0026lt;set\u0026gt; using namespace std; int main() { std::set\u0026lt;int\u0026gt; testSet; testSet.insert(20); testSet.insert(10); //iteratorz，迭代器，用于遍历容器内元素和元素数据类型 for (set\u0026lt;int\u0026gt;::iterator i = testSet.begin(); i != testSet.end(); i++) { cout \u0026lt;\u0026lt; *i \u0026lt;\u0026lt; \u0026quot; \u0026quot;; } cout \u0026lt;\u0026lt; endl; return 0; }  输出：\n10 20  2.定义比较函数对自定义结构体(类)排序 2.1重载操作符 \u0026lt; 不能重载\u0026lt;=或\u0026gt;=，几乎所有的方法或容器都需要排序来满足数学意义上的标准严格弱序化，否则这些方法或容器的行为将不可预知。\n严格弱序化需要满足：\n   关系 说明     f(x,x) = false 相同为假   if f(x,y) then !f(y,x) 如果 x \u0026lt; y(x \u0026gt; y) 为真，则 x \u0026gt; y (x \u0026lt; y) 为假   if f(x,y) and f(y,z) then f(x,z) 如果 x \u0026gt; y (x \u0026lt; y) 且 y \u0026gt; z (y \u0026lt; z)，则x \u0026gt; z(x \u0026lt; z)   if !f(x,y)\u0026amp;\u0026amp;!f(y,x) then x==y; if x==y and y==z then x==z 如果 x \u0026lt; y 为假且 x \u0026gt; y 为假，则 x = y    set容器在判定已有元素a和新插入元素b是否相等时：\n 1.将a作为左操作数，b为右操作数，调用比较函数，得到返回值 2.将b作为左操作数，a为右操作数，再调用一次比较函数，得到返回值 3.如果前两步返回值都为false，则a=b，b不会被插入set容器；如果前两步返回值都为true，则可能得到不可预知的结果。  由上述，必须满足：比较函数对相同元素返回false\n示例，对结构体Stu按照id排序，如果id相同，对name按照字典序排序，同时去重(c++11)：\n#include \u0026lt;cstring\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;set\u0026gt; struct Stu { int id; const char *name; Stu(int d, const char *n) : id(d), name(n) {} Stu() {} //重载运算符'\u0026lt;' bool operator\u0026lt;(const Stu \u0026amp;right) const { if (id != right.id) return id \u0026lt; right.id; else { //id相同，根据name按照字典序排序，同时去重 if (strcmp(this-\u0026gt;name, right.name) == 0) return false; else return strcmp(this-\u0026gt;name, right.name) \u0026lt; 0; } } }; int main() { std::set\u0026lt;Stu\u0026gt; testSet; testSet.insert(Stu(2, \u0026quot;zhang\u0026quot;)); testSet.insert(Stu(2, \u0026quot;li\u0026quot;)); testSet.insert(Stu(2, \u0026quot;li\u0026quot;)); //重复数据 //对应insert，先调用存储对象构造函数，在内存中生成对象，然后拷贝至容器中，c++98不支持 testSet.emplace(1, \u0026quot;wang\u0026quot;); for (auto i : testSet) { std::cout \u0026lt;\u0026lt; \u0026quot;id: \u0026quot; \u0026lt;\u0026lt; i.id \u0026lt;\u0026lt; \u0026quot; \u0026quot; \u0026lt;\u0026lt; \u0026quot;name: \u0026quot; \u0026lt;\u0026lt; i.name \u0026lt;\u0026lt; std::endl; } return 0; }  Microsoft Visual C++ 6.0(c++98)：\n#include \u0026lt;string.h\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;set\u0026gt; using namespace std; struct Stu { int id; const char *name; Stu(int d, const char *n) : id(d), name(n) {} Stu() {} //重载运算符'\u0026lt;' bool operator\u0026lt;(const Stu \u0026amp;right) const { if (id != right.id) return id \u0026lt; right.id; else { //id相同，根据name按照字典序排序，同时去重 if (strcmp(this-\u0026gt;name, right.name) == 0) return false; else return strcmp(this-\u0026gt;name, right.name) \u0026lt; 0; } } }; int main() { set\u0026lt;Stu\u0026gt; testSet; testSet.insert(Stu(2, \u0026quot;zhang\u0026quot;)); testSet.insert(Stu(2, \u0026quot;li\u0026quot;)); testSet.insert(Stu(2, \u0026quot;li\u0026quot;)); testSet.insert(Stu(1, \u0026quot;wang\u0026quot;)); for (set\u0026lt;Stu\u0026gt;::iterator i = testSet.begin(); i != testSet.end(); i++) { cout \u0026lt;\u0026lt; \u0026quot;id: \u0026quot; \u0026lt;\u0026lt; i-\u0026gt;id \u0026lt;\u0026lt; \u0026quot; \u0026quot; \u0026lt;\u0026lt; \u0026quot;name: \u0026quot; \u0026lt;\u0026lt; i-\u0026gt;name \u0026lt;\u0026lt; endl; } return 0; }  输出：\nid: 1 name: wang id: 2 name: li id: 2 name: zhang  2.2重载操作符 () ","date":1552552365,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552552365,"objectID":"8c6b84262df08234e788d47c0ec9a747","permalink":"https://blog.codist.me/en/docs/stl-set/","publishdate":"2019-03-14T16:32:45+08:00","relpermalink":"/en/docs/stl-set/","section":"docs","summary":"std::set，是基于红黑树的平衡二叉树的数据结构实现的一种容器，因为其中所包含的元素的值是唯一的，因此主要用于排序和去重。 1.使用内置的","tags":null,"title":"SET容器 - 自定义排序和去重","type":"docs"},{"authors":null,"categories":null,"content":" 获取Homo运行所需的数据\n1. 离线唤醒部分 1.1 中文语音模型 这里直接使用官方训练好的中文普通话模型，之后再讨论如何自己训练\n下载 中文(普通话)预训练语音模型\n解压到 homo/sphinx/cmusphinx-zh-cn-5.2\n1.2 英文语音模型 Homo 默认使用的唤醒词是 homo，需要使用英文语音模型\n在安装 PocketSphinx 时，git clone 得到的源码里已经包含训练好的模型文件，直接将文件夹 pocketsphinx/model/en-us 拷贝到 homo/sphinx/ 里：\ncp -r pocketsphinx/model/en-us homo/sphinx/  2. 自然语言理解部分 2.1 中文维基百科MITIE模型 来自 用Rasa NLU构建自己的中文NLU系统 - 羊肉泡馍与糖蒜\n下载 total_word_feature_extractor_chi.dat:\n链接：https://pan.baidu.com/s/1kNENvlHLYWZIddmtWJ7Pdg 密码：p4vx  放置到 homo/nlu/data/rasa/total_word_feature_extractor_chi.dat\n","date":1560355200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560441600,"objectID":"3159f8bfc3a1e5c9d1f609b9f452b21a","permalink":"https://blog.codist.me/en/homo/dataset/","publishdate":"2019-06-13T00:00:00+08:00","relpermalink":"/en/homo/dataset/","section":"homo","summary":"获取Homo运行所需的数据 1. 离线唤醒部分 1.1 中文语音模型 这里直接使用官方训练好的中文普通话模型，之后再讨论如何自己训练 下载 中文(普通话)预训练","tags":null,"title":"获取数据","type":"docs"},{"authors":null,"categories":null,"content":" 首先，在产品目录的init.XXX.rc文件中，添加相应的service，\n#start log service start logd on property:service.logcat.enable=1 start logcat_service on property:service.logcat.enable=0 stop logcat_service #log services service logcat_service /system/bin/logcat -b system -b events -b main -b radio -k -n 10 -v threadtime -r5000 -f /data/Logs/Log.0/logcat.log user root group log system class main disabled service logd /system/bin/sh /system/bin/logd.sh user system group log oneshot  然后，在目标平台的system/bin下添加脚本文件logd.sh，处理存储的log日志，以及设置属性，开启logcat_service,\n#!/system/bin/sh # #Global folder \u0026amp; cmd params # OUTPUT_DIR=/data LOG=Logs index=2 LOG_DIR[0]=$OUTPUT_DIR/$LOG/Log.0 LOG_DIR[1]=$OUTPUT_DIR/$LOG/Log.1 LOG_DIR[2]=$OUTPUT_DIR/$LOG/Log.2 RM=rm MV=\u0026quot;mv\u0026quot; MKDIR=mkdir UMASK=umask #set default permission 0775 $UMASK 002 #Init the three folders i=0 while [ \u0026quot;$i\u0026quot; -le \u0026quot;$index\u0026quot; ] do $MKDIR -p ${LOG_DIR[$i]} i=$(($i+1)) done #Transfer the three folders ((i=$index-1)) $RM -r ${LOG_DIR[$index]}/* while [ \u0026quot;$i\u0026quot; -ge \u0026quot;0\u0026quot; ] do $MV ${LOG_DIR[$i]}/* ${LOG_DIR[$i+1]} i=$(($i-1)) done $RM -r ${LOG_DIR[0]}/* #start logcat service setprop service.logcat.enable 1 mkdir /data/www cp -R /system/var/www/ /data/ ln -s /storage/external/ /data/www/sdcard   ","date":1465642810,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1465642810,"objectID":"143d29d0a317cf7d35bf4f95f7fe8d1c","permalink":"https://blog.codist.me/en/docs/android-log/","publishdate":"2016-06-11T19:00:10+08:00","relpermalink":"/en/docs/android-log/","section":"docs","summary":"首先，在产品目录的init.XXX.rc文件中，添加相应的service， #start log service start logd on property:service.logcat.enable=1 start logcat_service on property:service.logcat.enable=0 stop logcat_service #log services service logcat_service /system/bin/logcat -b system -b events -b main -b radio -k -n 10 -v threadtime -r5000 -f","tags":["Android","log"],"title":"将Android的系统日志输出到文件","type":"docs"},{"authors":null,"categories":null,"content":" Homo 的示例配置文件在 conf/example_app.ini，在运行前，需要拷贝一份到项目根目录下的 app.ini:\ncd homo cp conf/example_app.ini conf/app.ini   注意：一般情况下不需要修改配置文件app.ini，程序在每次启动时都会对未填的项用默认配置填充   配置文件各参数介绍 [portaudio] ; PocketSphinx 将 PortAudio 录制到的原始音频(16kHz)存储在这个路径下 ; 默认值: tmp/record RAW_DIR = ; 保存用于语音识别的临时文件名 ; 默认值: tmp/record/input.pcm INPUT_RAW = ; 保存用于语音识别的临时文件编码成的wav文件名 ; 默认值: tmp/record/input.wav INPUT_WAV = [sphinx] ; 英文 hmm 模型路径 ; 默认值: sphinx/en-us/en-us EN_HMM_DIR = ; Homo唤醒词词典路径 ; 默认值: sphinx/homo/homo.dic EN_DICT_FILE = ; Homo lm 模型路径 ; 默认值: sphinx/homo/homo.lm.bin EN_LM_FILE = ; 在PocketSphinx基础上降低开始录制用于语音识别音频的阈值，越小越灵敏 ; 最小值为0，将完全由PocketSphinx控制 ; 默认值: 50000 RECORD_THRESHOLD = ; 保存PocketSphinx的日志文件名 ; 默认值: log/sphinx.log LOG_FILE = [nlu] ; Rasa NLU 对话服务的API(已弃用), ; 默认值: http://localhost:5005/conversations/default/respond CONVERSATION_API = ; Rasa NlU进行意图识别和实体提取的API, ; 默认值: http://localhost:5000/parse PARSE_API = ; 使用的Rasa NLU的模型所属项目 ; 默认值: rasa PROJECT = ; 使用的Rasa NLU的模型 ; 默认值: ini MODEL = [baidu] ; 百度语音识别的API ; 默认值: http://vop.baidu.com/server_api ASR_API = ; 百度语音合成的API ; 默认值: http://tsn.baidu.com/text2audio TTS_API = ; 百度语音平台的认证API ; 默认值: https://openapi.baidu.com/oauth/2.0/token VOICE_AUTH_URL = ; 百度语音API的key ; 由于免费API有并发限制，建议自己注册 ; 默认值: MDNsII2jkUtbF729GQOZt7FS VOICE_API_KEY = ; 百度语音API的secret ; 由于免费API有并发限制，建议自己注册 ; 默认值: 0vWCVCLsbWHMSH1wjvxaDq4VmvCZM2O9 VOICE_API_SECRET = [tts] ; 保存合成的临时语音文件的目录 ; 默认值: tmp/tts TTS_DIR = ; 保存合成的临时语音文件名 ; 默认值: tmp/tts/tmp.wav TTS_OUT_FILE =  ","date":1560355200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560441600,"objectID":"8d692fdec08685f8c3b57e6064860f29","permalink":"https://blog.codist.me/en/homo/config/","publishdate":"2019-06-13T00:00:00+08:00","relpermalink":"/en/homo/config/","section":"homo","summary":"Homo 的示例配置文件在 conf/example_app.ini，在运行前，需要拷贝一份到项目根目录下的 app.ini: cd homo cp conf/example_app.ini conf/app.ini 注意：一般情况下不需要修改配置","tags":null,"title":"配置","type":"docs"},{"authors":null,"categories":null,"content":" 1.运行自然语言理解引擎 进入nlu的文件夹，source对应的python虚拟环境并启动http服务器：\ncd nlu source env3.6/bin/activate python -m rasa_nlu.server \\ -c configs/rasa/config_jieba_mitie_sklearn.yml \\ --path models  或者直接运行脚本nlu_server.sh：\ncd nlu ./nlu_server.sh  2.运行文本情感分析引擎 进入sentiment文件夹，source对应的python虚拟环境并启动http服务器：\ncd sentiment source env3.6/bin/activate python server.py  或直接运行脚本：\ncd sentiment ./server.sh  注意：加载word2vec模型需要花费5~7分钟时间\n3.运行主程序 ./homo-webview  查看帮助使用 -h 或 --help：\n$ ./homo-webview -h NAME: Homo Webview - Help USAGE: homo-webview [global options] command [command options] [arguments...] VERSION: 0.0.1 COMMANDS: help, h Shows a list of commands or help for one command GLOBAL OPTIONS: --debug, -d start homo webview in debug mode [$HOMO_WEBVIEW_DEBUG] --offline, -o disable speech recognition and text to speech [$HOMO_WEBVIEW_OFFLINE] --fall, -f disable wakeup [$HOMO_WEBVIEW_FALL] --interrupt, -i can interrupt the playing voice [$HOMO_WEBVIEW_INTERRUPT] --help, -h show help --version, -v print the version  所有参数均可以叠加使用，如 ./homo-webview -o -f -d\n3.1 离线模式 使用 -o 或 --offline：\n./homo-webview -o  该模式下完全不需要网络，语音识别和语音合成功能关闭，只能通过键入文本交互\n3.2 无唤醒模式 使用 -f 或 --fall：\n./homo-webview -f  该模式下不需要唤醒，直接启动界面\n3.3 调试模式 使用 -d 或 --debug：\n./homo-webview -d  在该模式下，将会输出更为详细的日志信息，包括函数所在的代码文件和行号\n界面右键可以调用控制台查看和调试html和js/css\n","date":1560441600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560441600,"objectID":"15f1ba91a1151445071387cd55e98180","permalink":"https://blog.codist.me/en/homo/run/","publishdate":"2019-06-14T00:00:00+08:00","relpermalink":"/en/homo/run/","section":"homo","summary":"1.运行自然语言理解引擎 进入nlu的文件夹，source对应的python虚拟环境并启动http服务器： cd nlu source env3.6/bin/activate python -m rasa_nlu.server \\ -c configs/rasa/config_jieba_mitie_sklearn.yml \\ --path models 或者直接运","tags":null,"title":"运行","type":"docs"},{"authors":null,"categories":null,"content":" 环境 操作系统建议使用Ubuntu，因为官方源里已经有编译好的一些依赖库，如ATLAS，安装不会遇到太多问题。\n需要安装有git，subversion，make以及gcc等编译工具链\n1.下载源码 git clone https://github.com/kaldi-asr/kaldi.git  2.编译安装kaldi/tools 安装文档在kaldi/tools/INSTALL，很短，建议阅读一下\n开始编译：\ncd tools ./extras/check_dependencies.sh #检查依赖，如有问题参照仔细修改 #自动下载安装依赖 make -j 8   注意：如果要使用特定版本的编译器，必须和后面编译安装kaldi/src的编译器一致，如之后我们编译安装cuda要用g++-7，需要指定要用的编译器：\nCXX=g++-7 extras/check_dependencies.sh make CXX=g++-7 -j 8    3.编译安装kaldi/src 3.1使用OpenBLAS 如果使用Ubuntu系统，源里有编译好的ATLAS库，可以直接安装，其他Linux发行版由于ATLAS安装复杂，需要调节cpu工作模式，这里用OpenBLAS替代：\ncd tools ./extras/install_openblas.sh  3.2安装cuda thchs30和aishell的训练都需要用到cuda，建议在编译安装的时候就把相关依赖一并装好。\n可以手动在nvidia官网下载cuda工具集，根据对应的显卡和平台：https://developer.nvidia.com/cuda-downloads ，再手动安装。\nArchlinux可以直接从源里安装，Ubuntu等发行版类似：\nsudo pacman -S cuda  注意：对于Archlinux，由于源里的cuda更新比较频繁，编译安装的kaldi会依赖指定版本的cuda，如果cuda大版本更新，如10.0=\u0026gt;10.1，会使kaldi找不到旧版的cuda库而出错，建议添加cuda到忽略升级里，或每次更新cuda后重新编译安装kaldi/src\n3.3编译 根据安装文档下载编译依赖(这里使用OpenBLAS)，需要硬盘上有20G的空闲空间：\n./configure --openblas-root=../tools/OpenBLAS/install  如果没有找到cuda安装路径，提示：\nCUDA will not be used! If you have already installed cuda drivers and cuda toolkit, try using --cudatk-dir=... option. Note: this is only relevant for neural net experiments  按照提示指定cuda安装路径，Archlinux安装路径在/opt/cuda/：\n./configure --openblas-root=../tools/OpenBLAS/install --cudatk-dir=/opt/cuda/  如果提示g++版本问题：\nConfiguring dynamically loaded OpenBlas since --static-math=no (the default) Successfully configured for Linux with OpenBLAS from /home/countstarlight/data/Documents/kaldi/tools/OpenBLAS/install ***configure failed: CUDA 10_0 does not support g++ (g++-8.2.1). You need g++ \u0026lt; 8.0. ***  比如这里提示cuda10只支持g++8.0以下的版本，不需要降级，从源里安装低版本的g++，Archlinux会在安装cuda时自动安装适合版本的g++，Ubuntu等发行版类似，指定用低版本的g++，这里我们用g++-7：\nCXX=g++-7 ./configure --openblas-root=../tools/OpenBLAS/install --cudatk-dir=/opt/cuda/ --shared make depend -j 8 make -j 8  这样就已经编译好训练需要的工具，之后进行在线解码和处理一些数据集需要额外安装一些工具\n","date":1512230400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512230400,"objectID":"315b797382397d837bc704919f932874","permalink":"https://blog.codist.me/en/docs/kaldi-install/","publishdate":"2017-12-03T00:00:00+08:00","relpermalink":"/en/docs/kaldi-install/","section":"docs","summary":"环境 操作系统建议使用Ubuntu，因为官方源里已经有编译好的一些依赖库，如ATLAS，安装不会遇到太多问题。 需要安装有git，subvers","tags":["kaldi","ASR"],"title":"安装kaldi","type":"docs"},{"authors":null,"categories":null,"content":" 1.下载数据集 Kaldi中文语音识别公共数据集有：\n 1.aishell：AI SHELL公司开源178小时中文语音语料及基本训练脚本，见kaldi-master/egs/aishell\n 2.gale_mandarin：中文新闻广播数据集(LDC2013S08, LDC2013S08）\n 3.hkust：中文电话数据集(LDC2005S15, LDC2005T32)\n 4.thchs30：清华大学30小时的数据集，可以在http://www.openslr.org/18/ 下载\n  这里采用thchs30，从http://www.openslr.org/18/ 或者参照它的README下载三个压缩包：\n data_thchs30.tgz 6.4G Mirrors: China\n test-noise.tgz 1.9G Mirrors: China\n resource.tgz 24M Mirrors: China\n  在egs/thchs30/s5下新建文件夹thchs30-openslr，把三个文件解压在该文件夹下\n这个数据集包含以下内容：\n   数据集 音频时长(h) 句子数 词数     train(训练) 25 10000 198252   dev(开发) 2:14 893 17743   test(测试) 6:15 2495 49085    还有训练好的语言模型word.3gram.lm和phone.3gram.lm以及相应的词典lexicon.txt。\n其中dev的作用是在某些步骤与train进行交叉验证的，如local/nnet/run_dnn.sh同时用到exp/tri4b_ali和exp/tri4b_ali_cv。训练和测试的目标数据也分为两类：word（词）和phone（音素）。\n local/thchs-30_data_prep.sh：主要工作是从$thchs/data_thchs30（下载的数据）三部分分别生成word.txt（词序列），phone.txt（音素序列），text（与word.txt相同），wav.scp（语音），utt2pk（句子与说话人的映射），spk2utt（说话人与句子的映射）\n #produce MFCC features：提取MFCC特征，分为两步，先通过steps/make_mfcc.sh提取MFCC特征，再通过steps/compute_cmvn_stats.sh计算倒谱均值和方差归一化。\n #prepare language stuff：构建一个包含训练和解码用到的词的词典。而语言模型已经由王东老师处理好了，如果不打算改语言模型，这段代码也不需要修改。\n a)基于词的语言模型包含48k基于三元词的词，从gigaword语料库中随机选择文本信息进行训练得到，训练文本包含772000个句子，总计1800万词，1.15亿汉字 b)基于音素的语言模型包含218个基于三元音的中文声调，从只有200万字的样本训练得到，之所以选择这么小的样本是因为在模型中尽可能少地保留语言信息，可以使得到的性能更直接地反映声学模型的质量。 c)这两个语言模型都是由SRILM工具训练得到。   2.训练 2.1修改训练脚本 1.首先修改s5/cmd.sh脚本，把原脚本注释掉，修改为本地运行：\n#export train_cmd=queue.pl #export decode_cmd=\u0026quot;queue.pl --mem 4G\u0026quot; #export mkgraph_cmd=\u0026quot;queue.pl --mem 8G\u0026quot; #export cuda_cmd=\u0026quot;queue.pl --gpu 1\u0026quot; export train_cmd=run.pl export decode_cmd=\u0026quot;run.pl --mem 4G\u0026quot; export mkgraph_cmd=\u0026quot;run.pl --mem 8G\u0026quot; export cuda_cmd=\u0026quot;run.pl --gpu 1\u0026quot;  2.然后修改s5/run.sh脚本，需要修改两个地方：\n第一个地方是修改并行任务的数量，cpu核心数*2：\nn=8 #parallel jobs  第二个地方是修改数据集放的位置，修改为上面解压出的目录路径：\n#corpus and trans directory #thchs=/nfs/public/materials/data/thchs30-openslr thchs=/home/countstarlight/data/Documents/kaldi/egs/thchs30/s5/thchs30-openslr  2.2运行训练 ./run.sh  分为几个过程：数据准备，monophone单音素训练， tri1三因素训练， trib2进行lda_mllt特征变换，trib3进行sat自然语言适应，trib4做quick，后面就是dnn了，本地不建议跑dnn。\n 建议将run.sh的几个过程分成多个脚本文件来跑，每段脚本里并发数量n必须相同，有些任务是是并发的\u0026amp;，也就是当前脚本执行完也会继续进行，这些并发任务同样需要大量内存(5~8G)，建议用系统监视器监视当前内存占用，等内存恢复到正常水平后再进行下一段脚本。   exp目录是得到的结果，比如tri1，decode_test_word/scoring_kaldi/best_wer是它的错误率，36.15%。tri1下的final.mdl是得到的模型的链接文件，要用的是它链接到的具体文件。graph_word里的words.txt和HCLG.fst，一个是字典，一个是有限状态机。这3个文件用来识别。\n3.在线识别 这里的“在线”是指一句话还没有说完，即还没有将一句话完整的音频传入就开始识别。\n3.1安装PortAudio 1.先修改tools/extras/install_portaudio.sh，取消对jack的依赖：\n#./configure --prefix=`pwd`/install --with-pic ./configure --prefix=`pwd`/install --with-pic --without-jack  2.安装PortAudio：\ncd tools ./extras/install_portaudio.sh  3.编译扩展程序：\ncd src make ext -j 8  生成的文件在src/onlinebin，其中：\n online-wav-gmm-decode-faster ：从wav文件读取 online-gmm-decode-faster：从麦克风输入声音  4.测试一下麦克风是否正常：\narecord -f cd -r 16000 -d 5 test.wav  16位，16khz，录音5秒，保存文件为test.wav。\n3.2建立相关目录 复制官方online demo文件到thchs30目录，并建立目录：\ncp -r egs/voxforge/online_demo egs/thchs30/ cd egs/thchs30/online_demo #audio用于存放测试用的wav文件(16位，16khz) mkdir -p online-data/audio #从wav文件读取需要用到 touch online-data/audio/trans.txt #models用于存放模型文件 mkdir -p online-data/models/tri1  将s5/exp/tri1中训练得到的： * 35.mdl：模型文件，final.mdl链接到这个文件 * graph_word/words.txt：字典 * graph_word/HCLG.fst：有限状态机\n复制到online-data/models/tri1下。\n3.3修改脚本文件 1.编辑online_demo/run.sh，注释掉如下代码（这段是voxforge例子中下载测试语料和识别模型的。我们测试语料自己准备，模型就是tri1了）：\n#if [ ! -s ${data_file}.tar.bz2 ]; then # echo \u0026quot;Downloading test models and data ...\u0026quot; # wget -T 10 -t 3 $data_url; # # if [ ! -s ${data_file}.tar.bz2 ]; then # echo \u0026quot;Download of $data_file has failed!\u0026quot; # exit 1 # fi #fi  2.修改模型路径为tri1：\n#ac_model_type=tri2b_mmi ac_model_type=tri1  3.修改相关文件路径：\n# online-gmm-decode-faster --rt-min=0.5 --rt-max=0.7 --max-active=4000 \\ # --beam=12.0 --acoustic-scale=0.0769 $ac_model/model $ac_model/HCLG.fst \\ # $ac_model/words.txt '1:2:3:4:5' $trans_matrix;; online-gmm-decode-faster --rt-min=0.5 --rt-max=0.7 --max-active=4000 \\ --beam=12.0 --acoustic-scale=0.0769 $ac_model/35.mdl $ac_model/HCLG.fst \\ $ac_model/words.txt '1:2:3:4:5' $trans_matrix;;  # online-wav-gmm-decode-faster --verbose=1 --rt-min=0.8 --rt-max=0.85\\ # --max-active=4000 --beam=12.0 --acoustic-scale=0.0769 \\ # scp:$decode_dir/input.scp $ac_model/model $ac_model/HCLG.fst \\ # $ac_model/words.txt '1:2:3:4:5' ark,t:$decode_dir/trans.txt \\ # ark,t:$decode_dir/ali.txt $trans_matrix;; online-wav-gmm-decode-faster --verbose=1 --rt-min=0.8 --rt-max=0.85\\ --max-active=4000 --beam=12.0 --acoustic-scale=0.0769 \\ scp:$decode_dir/input.scp $ac_model/35.mdl $ac_model/HCLG.fst \\ $ac_model/words.txt '1:2:3:4:5' ark,t:$decode_dir/trans.txt \\ ark,t:$decode_dir/ali.txt $trans_matrix;;  3.4识别 从麦克风识别：\n./run.sh --test-mode live  如果提示portaudio错误，可参考https://blog.csdn.net/u012236368/article/details/71628777\n识别wav文件：\n./run.sh --test-mode simulated  3.5运行其他模型 tri2b（tri3和tri4同理），把s5/exp/tri2b下的12.mat，35.mdl复制到models/tri2b下，再拷贝其他相应的文件（同tri1的思路），所以/tri2目录下包括如下文件：12.mat、35.mdl、HCLG.fst、words.txt。接着修改run.sh：\n# Change this to \u0026quot;tri2a\u0026quot; if you like to test using a ML-trained model #ac_model_type=tri2b_mmi ac_model_type=tri2b  把12.mat引入命令中：\nac_model=${data_file}/models/$ac_model_type #trans_matrix=\u0026quot;\u0026quot; trans_matrix=\u0026quot;$ac_model/12.mat\u0026quot; audio=${data_file}/audio  添加2个参数--left-context=3 --right-context=3：\n# online-gmm-decode-faster --rt-min=0.5 --rt-max=0.7 --max-active=4000 \\ # --beam=12.0 --acoustic-scale=0.0769 $ac_model/model $ac_model/HCLG.fst \\ # $ac_model/words.txt '1:2:3:4:5' $trans_matrix;; online-gmm-decode-faster --rt-min=0.5 --rt-max=0.7 --max-active=4000 \\ --beam=12.0 --acoustic-scale=0.0769 --left-context=3 --right-context=3 $ac_model/35.mdl $ac_model/HCLG.fst \\ $ac_model/words.txt '1:2:3:4:5' $trans_matrix;;  # online-wav-gmm-decode-faster --verbose=1 --rt-min=0.8 --rt-max=0.85\\ # --max-active=4000 --beam=12.0 --acoustic-scale=0.0769 \\ # scp:$decode_dir/input.scp $ac_model/model $ac_model/HCLG.fst \\ # $ac_model/words.txt '1:2:3:4:5' ark,t:$decode_dir/trans.txt \\ # ark,t:$decode_dir/ali.txt $trans_matrix;; online-wav-gmm-decode-faster --verbose=1 --rt-min=0.8 --rt-max=0.85\\ --max-active=4000 --beam=12.0 --acoustic-scale=0.0769 --left-context=3 --right-context=3 \\ scp:$decode_dir/input.scp $ac_model/35.mdl $ac_model/HCLG.fst \\ $ac_model/words.txt '1:2:3:4:5' ark,t:$decode_dir/trans.txt \\ ark,t:$decode_dir/ali.txt $trans_matrix;;  从麦克风识别：\n./run.sh --test-mode live  识别wav文件：\n./run.sh --test-mode simulated  如果要运行dnn，首先要将nnet1转成nnet2。可以参考链接1和链接2。\n4.算法解读 1.首先用标准的13维MFCC加上一阶和二阶导数训练单音素GMM系统，采用倒谱均值归一化（CMN）来降低通道效应。然后基于具有由LDA和MLLT变换的特征的单音系统构造三音GMM系统，最后的GMM系统用于为随后的DNN训练生成状态对齐。\n2.基于GMM系统提供的对齐来训练DNN系统，特征是40维FBank，并且相邻的帧由11帧窗口（每侧5个窗口）连接。连接的特征被LDA转换，其中维度降低到200。然后应用全局均值和方差归一化以获得DNN输入。DNN架构由4个隐藏层组成，每个层由1200个单元组成，输出层由3386个单元组成。 基线DNN模型用交叉熵的标准训练。 使用随机梯度下降（SGD）算法来执行优化。 将迷你批量大小设定为256，初始学习率设定为0.008。\n3.被噪声干扰的语音可以使用基于深度自动编码器（DAE）的噪声消除方法。DAE是自动编码器（AE）的一种特殊实现，通过在模型训练中对输入特征引入随机破坏。已经表明，该模型学习低维度特征的能力非常强大，并且可以用于恢复被噪声破坏的信号。在实践中，DAE被用作前端管道的特定组件。输入是11维Fbank特征（在均值归一化之后），输出是对应于中心帧的噪声消除特征。然后对输出进行LDA变换，提取全局标准化的常规Fbank特征，然后送到DNN声学模型（用纯净语音进行训练）。\n训练与解码脚本解读\n本节结合官方文档对主要脚本进行解读。 以下流程中的符号解释：-\u0026gt;表示下一步，{}表示循环，[]表示括号内每一个都要进行一次，()表示不同分支下可能进行的操作 1.train_mono.sh 用来训练单音子隐马尔科夫模型，一共进行40次迭代，每两次迭代进行一次对齐操作\ngmm-init-mono-\u0026gt;compile-train-graphs-\u0026gt;align-equal-compiled-\u0026gt;gmm-est-\u0026gt; {gmm-align-compiled-\u0026gt;gmm-acc-stats-ali-\u0026gt;gmm-est}40-\u0026gt;analyze_alignments.sh\n2.train_deltas.sh 用来训练与上下文相关的三音子模型\ncheck_phones_compatible.sh-\u0026gt;acc-tree-stats-\u0026gt;sum-tree-stats-\u0026gt;cluster-phones-\u0026gt;compile-questions-\u0026gt; build-tree-\u0026gt;gmm-init-model-\u0026gt;gmm-mixup-\u0026gt;convert-ali-\u0026gt;compile-train-graphs-\u0026gt; {gmm-align-compiled-\u0026gt;gmm-acc-stats-ali-\u0026gt;gmm-est}35-\u0026gt;analyze_alignments.sh\n3.train_lda_mllt.sh 用来进行线性判别分析和最大似然线性转换\ncheck_phones_compatible.sh-\u0026gt;split_data.sh-\u0026gt;ali-to-post-\u0026gt;est-lda-\u0026gt;acc-tree-stats-\u0026gt;sum-tree-stats-\u0026gt; cluster-phones-\u0026gt;compile-questions-\u0026gt;build-tree-\u0026gt;gmm-init-model-\u0026gt;convert-ali-\u0026gt;compile-train-graphs-\u0026gt; {gmm-align-compiled-\u0026gt;gmm-acc-stats-ali-\u0026gt;gmm-est}35-\u0026gt;analyze_alignments.sh\n4.train_sat.sh 用来训练发音人自适应，基于特征空间最大似然线性回归\ncheck_phones_compatible.sh-\u0026gt;ali-to-post-\u0026gt;acc-tree-stats-\u0026gt;sum-tree-stats-\u0026gt;cluster-phones-\u0026gt;compile-questions-\u0026gt; build-tree-\u0026gt;gmm-init-model-\u0026gt;gmm-mixup-\u0026gt;convert-ali-\u0026gt;compile-train-graphs-\u0026gt; {gmm-align-compiled-\u0026gt;(ali-to-post-\u0026gt;)gmm-acc-stats-ali-\u0026gt;gmm-est}35-\u0026gt;ali-to-post-\u0026gt; gmm-est-\u0026gt;analyze_alignments.sh\n5.train_quick.sh 用来在现有特征上训练模型。 对于当前模型中在树构建之后的每个状态，它基于树统计中的计数的重叠判断的相似性来选择旧模型中最接近的状态。\ncheck_phones_compatible.sh-\u0026gt;ali-to-post-\u0026gt;est-lda-\u0026gt;acc-tree-stats-\u0026gt;sum-tree-stats-\u0026gt; cluster-phones-\u0026gt;compile-questions-\u0026gt;build-tree-\u0026gt;gmm-init-model-\u0026gt;convert-ali-\u0026gt;compile-train-graphs-\u0026gt; {gmm-align-compiled-\u0026gt;gmm-acc-stats-ali-\u0026gt;gmm-est}20-\u0026gt;analyze_alignments.sh\n6.run_dnn.sh 用来训练DNN，包括xent和MPE，\n{make_fbank.sh-\u0026gt;compute_cmvn_stats.sh}[train,dev,test]-\u0026gt;train.sh-\u0026gt;{decode.sh}[phone,word]-\u0026gt; align.sh-\u0026gt;make_denlats.sh-\u0026gt;train_mpe.sh-\u0026gt;{{decode.sh}[phone,word]}3\n7.train_mpe.sh 用来训练dnn的序列辨别MEP/sMBR。 这个阶段训练神经网络以联合优化整个句子，这比帧级训练更接近于一般ASR目标。 sMBR的目的是最大化从参考转录对齐导出的状态标签的期望正确率，而使用网格框架来表示竞争假设。 训练使用每句迭代的随机梯度下降法。 首先使用固定的低学习率1e-5（sigmoids）运行3-5轮。 在第一轮迭代后重新生成词图，我们观察到快速收敛。 我们支持MMI, BMMI, MPE 和sMBR训练。所有的技术在Switchboard 100h集上是相同的，仅仅在sMBR好一点点。 在sMBR优化中，我们在计算近似正确率的时候忽略了静音帧。\n{nnet-train-mpe-sequential}3-\u0026gt;make_priors.sh\n8.train_dae.sh 用来实验基于dae的去噪效果\ncompute_cmvn_stats.sh-\u0026gt;{add-noise-mod.py-\u0026gt;make_fbank.sh-\u0026gt;compute_cmvn_stats.sh}[train,dev,test]-\u0026gt; train.sh-\u0026gt;nnet-concat-\u0026gt;{{decode.sh}[phone,word]}[train,dev,test]\n9.train.sh 用来训练深度神经网络模型，帧交叉熵训练，该相位训练将帧分类为三音状态的DNN。这是通过小批量随机梯度下降完成的。 默认使用Sigmoid隐藏单元，Softmax输出单元和完全连接的AffineTransform层，学习率是0.008，小批量的大小为256。 我们没有使用动量或正则化（注：最佳学习率和隐藏单元的类型不同，sigmoid的值为0.008,tanh为0.00001。 通过‘–feature-transform’和‘-dbn’将input——transform和预训练的DBN传入此脚本，只有输出层被随机初始化。 我们使用提前停止来防止过度拟合，为此我们测量交叉验证集合（即保持集合）上的目标函数， 因此需要两对特征对齐dir来执行监督训练\nfeat-to-dim-\u0026gt;nnet-initialize-\u0026gt;compute-cmvn-stats-\u0026gt;nnet-forward-\u0026gt;nnet-concat-\u0026gt;cmvn-to-nnet-\u0026gt; feat-to-dim-\u0026gt;apply-cmvn-\u0026gt;nnet-forward-\u0026gt;nnet-initialize-\u0026gt;train_scheduler.sh\n10.train_scheduler.sh 典型的情况就是，train_scheduler.sh被train.sh调用。 一开始需要在交叉验证集上运行，主函数需要根据$iter来控制迭代次数和学习率。 学习率会随着目标函数相对性的提高而变化： 如果提高大于’start_halving_impr=0.01’，初始化学习率保持常数 否则学习率在每次迭代中乘以’halving_factor=0.5’来缩小 最后，如果提高小于’end_halving_impr=0.001’，训练终止。\n11.mkgraph.sh 用来建立一个完全的识别网络 12.decode.sh 用来解码并生成词错率结果 13.align_si.sh 对制定的数据进行对齐，作为新模型的输入 14.make_fmllr_feats.sh 用来保存FMLLR特征 15.pretrain_dbn.sh 深度神经网络预训练脚本 16.decode_fmllr.sh 对发音人自适应的模型进行解码操作 17.nnet-train-frmshuff.cc 最普遍使用的神经网络训练工具，执行一次迭代训练。过程： –feature-transform 即时特征扩展 NN输入-目标对的每帧重排 小批量随机梯度下降（SGD）训练 支持的每帧目标函数（选项 - 对象函数）： Xent：每帧交叉熵 Mse：每帧均方误差 18.nnet-forward.cc 通过神经网络转发数据，默认使用CPU。选项： –apply-log :产生神经网络的对数输出(比如：得到对数后验概率) –no-softmax :从模型中去掉soft-max层 —class-frame-counts：从声学得分中减去计算对数的计数\n专有缩写中文解释\ncmvn：倒谱均值和方差归一化 fft：快速傅里叶变换 GMM：高斯混合模型 MFCC：梅尔倒谱系数 pcm：脉冲编码调制 pdf：概率分布函数 PLP：感知线性预测系数 SGMM：子空间高斯混合模型 UBM：通用背景模型 VTLN：特征级声道长度归一化\n","date":1460908800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1460908800,"objectID":"2096de18557dd925e06f3adfe99e28c2","permalink":"https://blog.codist.me/en/docs/kaldi-thchs30/","publishdate":"2016-04-18T00:00:00+08:00","relpermalink":"/en/docs/kaldi-thchs30/","section":"docs","summary":"1.下载数据集 Kaldi中文语音识别公共数据集有： 1.aishell：AI SHELL公司开源178小时中文语音语料及基本训练脚本，见kald","tags":["kaldi","ASR"],"title":"使用thchs30数据集","type":"docs"},{"authors":null,"categories":null,"content":" 1.安装依赖 安装train_lm.sh：\ncd tools ./extras/install_kaldi_lm.sh  2.获取数据集 和thchs30类似，参照egs/aishell/README.txt，手动下载数据集或运行s5/run.sh会自动下载并解压缩数据集，这里只演示手动下载数据集。\n访问http://www.openslr.org/33/ ，下载两个压缩包：\ndata_aishell.tgz [15G] ( speech data and transcripts ) Mirrors: China\nresource_aishell.tgz [1.2M] ( supplementary resources, incl. lexicon, speaker info ) Mirrors: China\n下载解压缩到一个目录，这里解压缩到aishell-openslr\naishell-openslr/data_aishell/wav里的压缩包需要都解压出来，创建脚本local/untar.sh：\n#!/bin/bash remove_archive=false if [ \u0026quot;$1\u0026quot; == --remove-archive ]; then remove_archive=true shift fi if [ $# -ne 3 ]; then echo \u0026quot;Usage: $0 \u0026lt;data-base\u0026gt;\u0026quot; echo \u0026quot;e.g.: $0 /export/a05/xna/data\u0026quot; fi data=$1 part=\u0026quot;data_aishell\u0026quot; if [ ! -d \u0026quot;$data\u0026quot; ]; then echo \u0026quot;$0: no such directory $data\u0026quot; exit 1; fi if [ -f $data/$part/.complete ]; then echo \u0026quot;$0: data part $part was already successfully extracted, nothing to do.\u0026quot; exit 0; fi touch $data/$part/.complete if [ $part == \u0026quot;data_aishell\u0026quot; ]; then cd $data/$part/wav for wav in ./*.tar.gz; do echo \u0026quot;Extracting wav from $wav\u0026quot; tar -zxf $wav \u0026amp;\u0026amp; rm $wav done fi echo \u0026quot;$0: Successfully downloaded and un-tarred $data/$part.tgz\u0026quot; exit 0;  修改训练脚本run.sh：\n修改数据集存放路径\n#data=/export/a05/xna/data data=aishell-openslr #替换为你解压缩出数据集的路径 data_url=www.openslr.org/resources/33  修改下载解压缩数据集为我们新建的解压缩脚本\n#local/download_and_untar.sh $data $data_url data_aishell || exit 1; #local/download_and_untar.sh $data $data_url resource_aishell || exit 1; local/untar.sh $data || exit 1;  这样在首次运行训练脚本时会执行我们的解压缩脚本，对wav文件进行批量解压缩\n3.训练 3.1修改训练脚本 修改cmd.sh：\n#export train_cmd=\u0026quot;queue.pl --mem 2G\u0026quot; #export decode_cmd=\u0026quot;queue.pl --mem 4G\u0026quot; #export mkgraph_cmd=\u0026quot;queue.pl --mem 8G\u0026quot; export train_cmd=run.pl export decode_cmd=\u0026quot;run.pl --mem 4G\u0026quot; export mkgraph_cmd=\u0026quot;run.pl --mem 8G\u0026quot;  和thchs30一样，建议对训练脚本run.sh分成多个文件，run1.sh\u0026hellip;，分次执行，公共的内容是：\n#data=/export/a05/xna/data data=aishell-openslr #替换为你解压缩出数据集的路径 data_url=www.openslr.org/resources/33 . ./cmd.sh  3.2训练 ./run.sh # run1.sh run2.sh ...  4.识别\u0026ndash;在线解码 以aishell的chain模型为例\n4.1生成配置文件 所有操作都在aishell/s5下\n4.1.1使用nnet3模型 1.建立需要的软链接：\ncd exp/nnet3/tdnn_sp ln -s 0.mdl final.mdl  2.生成配置文件\n./steps/online/nnet3/prepare_online_decoding.sh \\ --add-pitch true \\ --mfcc-config conf/mfcc_hires.conf \\ data/lang \\ exp/nnet3/extractor \\ exp/nnet3/tdnn_sp \\ exp/nnet3/nnet_online   --add-pitch：aishell的nnet3和chain模型输入的特征在MFCC的基础上还加入了pitch特征，反应了音高的信息\n --mfcc-config：mfcc使用的配置文件，注意这里我们使用的是mfcc_hires.conf\n data/lang：nnet3模型解码网络图中G.fst和L.fst文件以及词汇表words.txt文件\n exp/nnet3/tdnn_sp：nnet3模型\n exp/nnet3/nnet_online：生成的配置文件存放的目录，自己定义\n  4.1.2使用chain模型(未经测试) ./steps/online/nnet3/prepare_online_decoding.sh \\ --add-pitch true \\ --mfcc-config conf/mfcc_hires.conf \\ data/lang \\ exp/nnet3/extractor \\ exp/chain/tdnn_1a_sp \\ exp/chain/nnet_online   --add-pitch：aishell的nnet3和chain模型输入的特征在MFCC的基础上还加入了pitch特征，反应了音高的信息\n --mfcc-config：mfcc使用的配置文件，注意这里我们使用的是mfcc_hires.conf\n data/lang_chain：存储chain模型解码网络图中G.fst和L.fst文件以及词汇表words.txt文件\n exp/chain/tdnn_1a_sp：存储chain模型\n exp/chain/nnet_online：生成的配置文件存放的目录，自己定义\n  4.2解码 4.2.1使用nnet3模型(未经测试) online2-wav-nnet3-latgen-faster \\ --config=exp/nnet3/nnet_online/conf/online.conf \\ --do-endpointing=false \\ --frames-per-chunk=20 \\ --extra-left-context-initial=0 \\ --online=true \\ --frame-subsampling-factor=3 \\ --min-active=200 \\ --max-active=7000 \\ --beam=15.0 \\ --lattice-beam=6.0 \\ --acoustic-scale=1.0 \\ --word-symbol-table=s5/data/lang/words.txt \\ s5/exp/chain/tdnn_1a_sp/final.mdl \\ s5/exp/chain/tdnn_1a_sp/graph/HCLG.fst \\ ark:s5/data/test/spk2utt \\ scp:s5/data/test/wav.scp \\ ark,t:20190308.txt  4.2.2使用chain模型(未经测试) online2-wav-nnet3-latgen-faster \\ --config=s5/exp/chain/nnet_online/conf/online.conf \\ --do-endpointing=false \\ --frames-per-chunk=20 \\ --extra-left-context-initial=0 \\ --online=true \\ --frame-subsampling-factor=3 \\ --min-active=200 \\ --max-active=7000 \\ --beam=15.0 \\ --lattice-beam=6.0 \\ --acoustic-scale=1.0 \\ --word-symbol-table=s5/data/lang/words.txt \\ s5/exp/chain/tdnn_1a_sp/final.mdl \\ s5/exp/chain/tdnn_1a_sp/graph/HCLG.fst \\ ark:s5/data/test/spk2utt \\ scp:s5/data/test/wav.scp \\ ark,t:20190308.txt  ","date":1512230400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512230400,"objectID":"872ceeede9f19e4c24782d6394efb3fd","permalink":"https://blog.codist.me/en/docs/kaldi-aishell/","publishdate":"2017-12-03T00:00:00+08:00","relpermalink":"/en/docs/kaldi-aishell/","section":"docs","summary":"1.安装依赖 安装train_lm.sh： cd tools ./extras/install_kaldi_lm.sh 2.获取数据集 和thchs30类似，参照egs/aishell/README.txt，手动下载","tags":["kaldi","ASR"],"title":"使用aishell数据集","type":"docs"},{"authors":null,"categories":null,"content":" cvte开放了已经训练好的模型，不用再花费大量时间和算力去训练，但注意cvte没有开源数据集和模型配置\n获取模型 从 http://kaldi-asr.org/models/m2 下载 0002_cvte_chain_model.tar.gz(3.5G)\n解压缩到kaldi/egs下，注意kaldi/egs/换成安装kaldi对应的目录：\ntar -zxvf 0002_cvte_chain_model.tar.gz -C kaldi/egs/  解压生成目录kaldi/egs/cvte，按照cvte/README.txt链接steps，utils和score.sh，由于需要修改utils中的脚本，这里直接拷贝utils文件夹：\ncd kaldi/egs/cvte/s5/ ln -s ../../wsj/s5/steps steps #ln -s ../../wsj/s5/utils utils cp -r ../../wsj/s5/utils utils cd local/ ln -s ../steps/score_kaldi.sh score.sh  修改解码脚本 修改utils/lang/check_phones_compatible.sh为：\n# check if the files exist or not if [ ! -f $table_first ]; then if [ ! -f $table_second ]; then echo \u0026quot;$0: Error! Both of the two phones-symbol tables are absent.\u0026quot; echo \u0026quot;Please check your command\u0026quot; #exit 1; 这里注释掉 else # The phones-symbol-table1 is absent. The model directory maybe created by old script. # For back compatibility, this script exits silently with status 0. exit 0; fi elif [ ! -f $table_second ]; then # The phones-symbol-table2 is absent. The model directory maybe created by old script. # For back compatibility, this script exits silently with status 0. exit 0; fi  运行解码测试 chmod +x *.sh ./run.sh  需要在大内存平台上，在本地12G内存的计算机上由于内存不够而出错\n","date":1512230400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512230400,"objectID":"8af34ebffa7ac419cbbb17a6742d9e21","permalink":"https://blog.codist.me/en/docs/kaldi-cvte/","publishdate":"2017-12-03T00:00:00+08:00","relpermalink":"/en/docs/kaldi-cvte/","section":"docs","summary":"cvte开放了已经训练好的模型，不用再花费大量时间和算力去训练，但注意cvte没有开源数据集和模型配置 获取模型 从 http://kaldi-asr.org/models/m2 下载 0002_cvte_chain_model.tar.gz(3.5G) 解压缩到kaldi/","tags":["kaldi","ASR"],"title":"使用cvte预训练模型","type":"docs"},{"authors":null,"categories":null,"content":" 在Linux上配置网络摄像头，用到的一些软件，以及如何录制和播放设备的视频输入\n1. 查找设备  插上摄像设备（通常是通过USB）\n 列出所有的 video4linux 设备:\n  ls -ltr /dev/video*   得到的输出类似于：  crw-rw----+ 1 root video 81, 0 Nov 11 09:06 /dev/video0  这里的摄像设备名称是 */dev/video0*，如果没有看到任何 /dev/video 文件，查看#排查问题。如果有多个 video4linux 设备，比如是一个tv card，摄像头设备应该显示为 /dev/video1 或类似的。但它的时间（在这个例子中是 Nov 11 09:06）应该是你插上它的时间。\n2.测试设备  如果安装有 vlc，可以启动它，选择 Media -\u0026gt; Open Capture Device -\u0026gt; Video device name = /dev/video0 -\u0026gt; Play\n 如果安装有mplayer，可以使用：\n  mplayer tv:// -tv driver=v4l2:width=640:height=480:device=/dev/video0 -fps 30  3.使用设备 3.1录制视频 要捕获设备的视频输入，可以使用 cheese，一个不错的图形界面软件，你可以用它边看边录制设备的视频输入，录制保存的文件格式不太常见(.webm)，但用vlc可以播放。\nYou can also automate video recording so you can capture the camera stream with sitting in front of the computer. To do this you can\n use the software mencoder:  mencoder tv:// -tv driver=v4l2:width=320:height=240:device=/dev/video0 -nosound -ovc lavc -o myvideo.avi   or use the software streamer. Here are two examples:  streamer -c /dev/video0 -f jpeg -F stereo -o myvideo.avi -t 0:05  3.2视频通话 视频通话，在Linux上使用skype.\n3.3查看视频输入 查看摄像设备的视频输入，使用cheese 或 mplayer:\nmplayer -fps 30 -cache 128 -tv driver=v4l2:width=640:height=480:device=/dev/video0 tv://  或者用vlc，你可以用root权限运行vlc，来查看你的摄像设备 /dev/video0，启动vlc并选择 Media -\u0026gt; Open Capture Device -\u0026gt; Video device name = /dev/video0 -\u0026gt; Play\n排查问题 Troubleshooting heavily depends on the distribution and version you are using. If you have done cabling correctly and a device file /dev/video* does not appear, your kernel probably does not know the hardware. In this case you may have to install the device driver separately because it may not be part of the kernel.\nSUSE Linux 11.0 and earlier This has been tested with SUSE Linux 11.0 x64 but should work with any earlier SUSE version. You will need to log in as user root. To find out what driver you need, open a console and call\nhwinfo --usb  If a Logitech Quickcam Messenger is plugged in the answer will be like:\n06: USB 00.2: 0000 Unclassified device [Created at usb.122] UDI: /org/freedesktop/Hal/devices/usb_device_46d_8da_noserial_if2 Unique ID: Eopr.vE+cdFBwClB Parent ID: uIhY.uOe2OKugI8D SysFS ID: /devices/pci0000:00/0000:00:1a.2/usb3/3-1/3-1:1.2 SysFS BusID: 3-1:1.2 Hardware Class: unknown Model: \u0026quot;Logitech QuickCam Messanger\u0026quot; Hotplug: USB Vendor: usb 0x046d \u0026quot;Logitech, Inc.\u0026quot; Device: usb 0x08da \u0026quot;QuickCam Messanger\u0026quot; Revision: \u0026quot;1.00\u0026quot; Driver: \u0026quot;snd-usb-audio\u0026quot; Driver Modules: \u0026quot;snd_usb_audio\u0026quot; Speed: 12 Mbps Module Alias: \u0026quot;usb:v046Dp08DAd0100dc00dsc00dp00ic01isc02ip00\u0026quot; Driver Info #0: Driver Status: quickcam_messenger is active Driver Activation Cmd: \u0026quot;modprobe quickcam_messenger\u0026quot; Driver Info #1: Driver Status: gspca is active Driver Activation Cmd: \u0026quot;modprobe gspca\u0026quot; Config Status: cfg=new, avail=yes, need=no, active=unknown Attached to: #20 (Hub)  This means you can install and load the webcam driver like this:\nyast -i gspcav-kmp-default modprobe gspca  Now you should see a video device:\nls /dev/video* /dev/video /dev/video0  That means you can install and start your webcam-viewer-software. We choose gqcam:\nyast -i gqcam gqcam  It works. You see a video what from what is going on in front of your webcam.\nUbuntu This has been tested with Ubuntu 8.10 x32 but should work with any Ubuntu version. Find out the driver activation command of your webcam. For this, first install the software hwinfo. Open a consoleand type:\nsudo apt-get install hwinfo  Then call hwinfo:\nhwinfo --usb  If a Logitech Quickcam Messenger is plugged in the response will be like:\n04: USB 00.2: 0000 Unclassified device [Created at usb.122] UDI: /org/freedesktop/Hal/devices/usb_device_46d_8da_noserial_if2 Unique ID: 4ajv.vE+cdFBwClB Parent ID: k4bc._Mkd+LmXb03 SysFS ID: /devices/pci0000:00/0000:00:11.0/0000:02:00.0/usb1/1-1/1-1:1.2 SysFS BusID: 1-1:1.2 Hardware Class: unknown Model: \u0026quot;Logitech QuickCam Messanger\u0026quot; Hotplug: USB Vendor: usb 0x046d \u0026quot;Logitech, Inc.\u0026quot; Device: usb 0x08da \u0026quot;QuickCam Messanger\u0026quot; Revision: \u0026quot;1.00\u0026quot; Driver: \u0026quot;snd-usb-audio\u0026quot; Driver Modules: \u0026quot;snd_usb_audio\u0026quot; Speed: 12 Mbps Module Alias: \u0026quot;usb:v046Dp08DAd0100dc00dsc00dp00ic01isc02ip00\u0026quot; Driver Info #0: Driver Status: gspca_zc3xx is active Driver Activation Cmd: \u0026quot;modprobe gspca_zc3xx\u0026quot; Config Status: cfg=new, avail=yes, need=no, active=unknown Attached to: #8 (Hub)  Activate the driver:\nsudo modprobe gspca_zc3xx  Now you should be able to see the video device:\nls /dev/video* /dev/video0  You can now test your webcam using the software cheese:\nsudo apt-get install cheese cheese  Other webcams If you have another webcam, try the above nevertheless. If it does not work, exchange the driver gspca against uvcvideo:\nyast -i uvcvideo_kmp_default modprobe uvcvideo  and start gqcam again.\nTestbed The following webcams have been found working with this tutorial:\n Logitech Quickcam messenger Philips Webcam SPC220NC  A general list of working webcams can be found at http://mxhaard.free.fr/spca5xx.html.\nThe guide has been tested with SUSE Linux 11.4 till 13.2 and Ubuntu.\nSee also  hardware http://en.opensuse.org/Webcam http://www.linux.com/feature/126186 http://ubuntulinuxhelp.com/linux-driver-for-quickcam-usb-cameras-logitech-quickcam-fusion/ http://www.goldmann.de/webcam-linux_tipp_408.html http://wiki.ubuntuusers.de/Webcam  ","date":1500533675,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1500533675,"objectID":"cff4d2264d95f65092227de68cf6974f","permalink":"https://blog.codist.me/en/docs/linux-webcam/","publishdate":"2017-07-20T14:54:35+08:00","relpermalink":"/en/docs/linux-webcam/","section":"docs","summary":"在Linux上配置网络摄像头，用到的一些软件，以及如何录制和播放设备的视频输入 1. 查找设备 插上摄像设备（通常是通过USB） 列出所有的 video4linux 设备: ls","tags":null,"title":"Linux上摄像设备的使用","type":"docs"},{"authors":null,"categories":null,"content":" 用途 xrandr 用于控制输出设备（显示器，投影仪等）的屏幕分辨率和方向（旋转角度）\n在主流Linux发行版上，都可以在桌面环境的系统设置里修改输出设备的设置，如kde的系统设置(System Setting)的显示和监视器(Display and Monitor)里，可以直接修改输出设备的显示方式，分辨率等\n也可以通过 快捷键 Win + P，唤起快速配置界面，选择输出设备的显示方式（复制，扩展，关闭其中一个输出设备等）\n常见问题  注意！！！\nxrandr会保存你的设置，这意味着，如果你在使用配置界面或快速配置界面设置了对一个输出方式的配置，如 HDMI-1的配置，下一次连接到该输出方式后，保存的配置即生效\n  这会导致一个非常严重的问题，如果你在首次使用某一个设备时配置失误，下次连接到该设备时，错误的配置项会立即生效\n在生产环境，如工作中使用投影仪进行屏幕共享时，请多加注意，这可能导致你在整个演示时间无法分享屏幕\n配置 在这里，我们讨论如何配置或修改 xrandr 的配置\n首先，xrandr 没有配置文件\n","date":1561221079,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561221079,"objectID":"66d752c99743bd15a0e7731646a5536f","permalink":"https://blog.codist.me/en/docs/xrandr/","publishdate":"2019-06-23T00:31:19+08:00","relpermalink":"/en/docs/xrandr/","section":"docs","summary":"用途 xrandr 用于控制输出设备（显示器，投影仪等）的屏幕分辨率和方向（旋转角度） 在主流Linux发行版上，都可以在桌面环境的系统设置里修改输出设备的","tags":null,"title":"屏幕输出管理: Xrandr","type":"docs"},{"authors":null,"categories":null,"content":"在Debian9上用开源的Commento搭建博客评论系统\n安装 yarn：\ncurl -sS https://dl.yarnpkg.com/debian/pubkey.gpg | sudo apt-key add - echo \u0026quot;deb https://dl.yarnpkg.com/debian/ stable main\u0026quot; | sudo tee /etc/apt/sources.list.d/yarn.list sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install yarn  安装dep（需要已经配置好的go环境）：\ngo get -u github.com/golang/dep/cmd/dep export PATH=$PATH:$GOPATH/bin  下载\u0026amp;编译：\nmkdir -p $GOPATH/gitlab.com/commento cd $GOPATH/gitlab.com/commento git clone https://gitlab.com/commento/commento.git make prod  在根目录下新建脚本server.sh：\n#! /bin/bash # 绑定的域名 export COMMENTO_ORIGIN=https://comment.example.com # 绑定的端口，可以用nginx反向代理到这个端口 export COMMENTO_PORT=8002 # 数据库 PostgreSQL 设置 export COMMENTO_POSTGRES=postgres://postgres:password@127.0.0.1:5432/comment?sslmode=disable # 关闭注册，首次需要注册账号，之后可以关闭注册 #export COMMENTO_FORBID_NEW_OWNERS=true # # github oauth，其他类似 # export GITHUB_KEY= export GITHUB_SECRET= ./build/prod/commento  运行：\nchmod +x server.sh ./server.sh  ","date":1461081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461081600,"objectID":"ba531e777d0e4eefd5795b3d60a9671e","permalink":"https://blog.codist.me/en/docs/selfhost-commento/","publishdate":"2016-04-20T00:00:00+08:00","relpermalink":"/en/docs/selfhost-commento/","section":"docs","summary":"在Debian9上用开源的Commento搭建博客评论系统 安装 yarn： curl -sS https://dl.yarnpkg.com/debian/pubkey.gpg | sudo apt-key add - echo \u0026quot;deb https://dl.yarnpkg.com/debian/ stable main\u0026quot; | sudo tee /etc/apt/sources.list.d/yarn.list sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install yarn 安装dep（需要已","tags":null,"title":"搭建博客评论系统","type":"docs"},{"authors":null,"categories":null,"content":" 1. 安装 minicom Debian/Ubuntu：\nsudo aptitude update sudo aptitude install minicom  2. 插上设备并查看系统是否已经检测到设备 sudo dmesg | grep tty  如果没有检测到设备，得到的结果类似： [ 0.000000] console [tty0] enabled [ 8.264501] systemd[1]: Created slice system-getty.slice.   拔下usb转串口线，输入命令 lsusb会看到一些已经连接到usb的设备：\nBus 002 Device 002: ID 8087:8000 Intel Corp. Bus 002 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub Bus 001 Device 002: ID 8087:8008 Intel Corp. Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub Bus 004 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub Bus 003 Device 004: ID 13d3:5188 IMC Networks Bus 003 Device 006: ID 13d3:3402 IMC Networks Bus 003 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub  再次插上usb转串口线，再次运行命令 lsusb，会看到输出结果相比之前增加了一行：\nBus 002 Device 002: ID 8087:8000 Intel Corp. Bus 002 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub Bus 001 Device 002: ID 8087:8008 Intel Corp. Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub Bus 004 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub Bus 003 Device 004: ID 13d3:5188 IMC Networks Bus 003 Device 006: ID 13d3:3402 IMC Networks Bus 003 Device 008: ID 18f8:0f99 --- --- --- (注意这行是新加的！) Bus 003 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub  现在我们知道了usb转串口的 vendor id 和 product id，让我们加载Linux内核的usbserial模块来激活这个设备：\n  sudo modprobe usbserial vendor=0x18f8 product=0x0f99   再次运行 dmesg 命令，输出结果类似：\nusbserial_generic 1-1:1.0: generic converter detected usb 1-1: generic converter now attached to ttyUSB0 usbcore: registered new interface driver usbserial_generic   可以把自动加载usbserial模块添加到开机启动里，编辑文件/etc/modules，添加一行：\n usbserial vendor=0x18f8 product=0x0f99  3. 连接到设备 假设设备路径为 /dev/ttyUSB0，运行命令:\nsudo minicom -s  在Serial port setup里修改第一行为/dev/ttyUSB0\n选择Save setup as dfl保存设置\n","date":1500946732,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1500946732,"objectID":"0603527b23ed53d6c2669ba19b71d3d6","permalink":"https://blog.codist.me/en/docs/embedded-serial-usb/","publishdate":"2017-07-25T09:38:52+08:00","relpermalink":"/en/docs/embedded-serial-usb/","section":"docs","summary":"1. 安装 minicom Debian/Ubuntu： sudo aptitude update sudo aptitude install minicom 2. 插上设备并查看系统是否已经检测到设备 sudo dmesg | grep tty 如果没有检测到设备，得到的结果类似： [ 0.000000] console","tags":["Linux","Serial-USB"],"title":"Linux上通过串口连接嵌入式Linux终端","type":"docs"},{"authors":null,"categories":null,"content":" 1. 设置文件路径  设置需要传送的文件所在的路径\n 按下组合键 Ctrl + A , 再按下 O\n 选择 Filenames and paths：\n    设置 A-Download directory （将文件从设备传送到本地的路径）\n 设置 B-Upload directory （将文件从本地上传到设备的路径）向设备发送文件时，选择这个目录里的文件。\n 按下 esc 然后选择 Save setup as dfl来保存设置：\n  2. 发送文件到设备  在minicom里（已经连接到设备的终端）  rx filename   按下组合键 Ctrl + A，然后按下S 选择 xmodem 选择要发送的文件  3. 在设备上运行刚才发送过去的文件（假设是Qt图形界面程序） 在minicom里（已经连接到设备的终端）：\nchmod +x filename #给文件授予执行权限 ./filename #运行  如果设备上的系统没有图形化环境 添加参数 -qws：\n./filename -qws  如果希望旋转屏幕显示 添加参数 -display 以及要旋转的角度（顺时针）：\n./filename -display \u0026quot;Transformed:Rot270\u0026quot;  ","date":1500946732,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1500946732,"objectID":"5ba45eb92504e3072a4e1ec0ac1d89b6","permalink":"https://blog.codist.me/en/docs/embedded-minicom/","publishdate":"2017-07-25T09:38:52+08:00","relpermalink":"/en/docs/embedded-minicom/","section":"docs","summary":"1. 设置文件路径 设置需要传送的文件所在的路径 按下组合键 Ctrl + A , 再按下 O 选择 Filenames and paths： 设置 A-Download directory （将文件从设备传送到本地的路径） 设置 B-Upload directory （将","tags":["Minicom","Linux"],"title":"用minicom通过串口发送文件","type":"docs"},{"authors":null,"categories":null,"content":" 1. 生成GPG key gpg --full-generate-key   加密方式选择RSA and RSA 过期时间输入：4096 valid选择默认  填写信息后生成成功。\n2.导出公钥 gpg --list-secret-keys --keyid-format LONG  rsa4096/后面的就是 GPG key ID\ngpg --armor --export \u0026lt;要导出的GPG Key ID\u0026gt;  将得到的一大段公钥粘贴到git平台。\n3.配置git 1.设置签名用的GPG key ID\ngit config --global user.signingkey \u0026lt;GPG Key ID\u0026gt;  2.设置gpg签名用的程序\ngit config --global gpg.program gpg  3.启用gpg签名\ngit config --global commit.gpgsign true  测试签名\necho \u0026quot;test\u0026quot; | gpg --clearsign  ","date":1552305830,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552305830,"objectID":"6626e7ebcb008db30757a7cb14670cd2","permalink":"https://blog.codist.me/en/docs/git-gpg/","publishdate":"2019-03-11T20:03:50+08:00","relpermalink":"/en/docs/git-gpg/","section":"docs","summary":"1. 生成GPG key gpg --full-generate-key 加密方式选择RSA and RSA 过期时间输入：4096 valid选择默认 填写信息后生成成功。 2.导出公钥 gpg --list-secret-keys --keyid-format LONG rsa4096/后面","tags":null,"title":"Git 用GPG签名","type":"docs"},{"authors":null,"categories":null,"content":" The following color themes are available and can be set by the color_theme option in config/_default/params.toml:\n          User Themes In this section, we will curate themes submitted by users. To create your own theme and request it to be added to this section, follow these steps:\n Follow the [guide to create a new theme]() Upload your theme file to a new Github repository Open a ticket with a link to your theme\u0026rsquo;s Github repository and a screenshot  ","date":1510761600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510761600,"objectID":"1093b6bde036f76f5e2a6fa578b9a639","permalink":"https://blog.codist.me/en/themes/","publishdate":"2017-11-16T00:00:00+08:00","relpermalink":"/en/themes/","section":"","summary":"Color themes for Academic.\n","tags":null,"title":"Themes","type":"page"}]